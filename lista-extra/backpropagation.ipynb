{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa função é usada para criar a base de treino do algoritmo. Assim ela é usada para definir as saidas lógicas baseado no tipo de gate escolhido. Se for \"AND\", a função testa de todos os valores são iguaus a 1, uma vez que a porta AND só retorna verdadeiro se todas as entradas forem verdadeiras. Se for \"OR\", a função testa se algum valor é 1, uma vez que a porta or retorna verdadeiro se alguma entrada for verdadeira. Se for \"XOR\", a função usa da função logical_xor do numpy para fazer a lógica xor. A função faz essa base com base nos inputs do tamanho determinado, onde cada linha representa uma combinação única de valores booleanos (0 e 1) para um conjunto de entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(num_inputs, logic_gate):\n",
    "    inputs = np.array(list(np.ndindex((2,) * num_inputs)))\n",
    "\n",
    "    if logic_gate.lower() == 'and':\n",
    "        outputs = np.all(inputs, axis=1).astype(int)\n",
    "    elif logic_gate.lower() == 'or':\n",
    "        outputs = np.any(inputs, axis=1).astype(int)\n",
    "    elif logic_gate.lower() == 'xor':\n",
    "        #print(\"1º ->\",inputs[:, 0])\n",
    "        #print(\"2º ->\",inputs[:, 1])\n",
    "\n",
    "        # XOR é definido como 1 apenas quando o número de 1s nas entradas é ímpar\n",
    "        outputs = np.logical_xor(inputs[:, 0], inputs[:, 1]).astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"Gate must be 'AND', 'OR', or 'XOR'.\")\n",
    "\n",
    "    return inputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Solicita ao usuário o número de entradas e o tipo de porta lógica\n",
    "    num_inputs = int(input(\"Digite o número de entradas (por exemplo, 2 ou 10): \"))\n",
    "    gate_type = input(\"Digite o tipo de porta lógica (AND, OR, ou XOR): \")\n",
    "\n",
    "    # Cria o conjunto de dados com base nas entradas do usuário\n",
    "    x, y = create_dataset(num_inputs, gate_type)\n",
    "    print(\"Base\\n - entradas\\n\",x,\" - saidas\",y, \"\\n\")\n",
    "\n",
    "    # Inicializa um classificador MLP (Multilayer Perceptron)\n",
    "    mlpR = MLPClassifier(activation=\"relu\", learning_rate=\"adaptive\")\n",
    "    mlpT = MLPClassifier(activation=\"tanh\", learning_rate=\"adaptive\")\n",
    "    mlpS = MLPClassifier(activation=\"logistic\", learning_rate=\"adaptive\")\n",
    "\n",
    "    # Treina o MLP usando os dados de entrada e saída\n",
    "    mlpR.fit(x, y)\n",
    "    mlpT.fit(x, y)\n",
    "    mlpS.fit(x, y)\n",
    "\n",
    "    # Testa o MLP com novas entradas fornecidas pelo usuário\n",
    "    while True:\n",
    "        user_inputs = [int(input(f\"Digite a entrada {i + 1} (0 ou 1): \")) for i in range(num_inputs)]\n",
    "        \n",
    "        # Faz uma previsão com o MLP treinado\n",
    "        predictionR = mlpR.predict([user_inputs])\n",
    "        predictionT = mlpT.predict([user_inputs])\n",
    "        predictionS = mlpS.predict([user_inputs])\n",
    "        \n",
    "        # Exibe o resultado da previsão\n",
    "        print(f\"Resultado:\\n - Relu:{predictionR}\\n - Tahn:{predictionT}\\n - Sigmore:{predictionS}\")\n",
    "        \n",
    "        predictionRa = mlpR.predict(x)\n",
    "        predictionTa = mlpT.predict(x)\n",
    "        predictionSa = mlpS.predict(x)\n",
    "        \n",
    "        acr = accuracy_score(y, predictionRa)\n",
    "        act = accuracy_score(y, predictionTa)\n",
    "        acs = accuracy_score(y, predictionSa)\n",
    "        print(f\"\\n accuracy\\n - Relu:{acr}\\n - Tahn:{act}\\n - Sigmore:{acs}\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análises\n",
    "### 1. Taxa de Aprendizado (`learning_rate`):\n",
    "\n",
    "A taxa de aprendizado é um hiperparâmetro crucial em algoritmos de otimização, como o usado no treinamento de redes neurais. Ela controla o tamanho dos passos que o otimizador dá ao ajustar os pesos durante o treinamento. A escolha adequada da taxa de aprendizado é essencial:\n",
    "\n",
    "- **Alta Taxa de Aprendizado:** Pode resultar em convergência mais rápida, mas também pode levar a oscilações e a overshooting, onde o algoritmo pode nunca convergir.\n",
    "- **Baixa Taxa de Aprendizado:** Pode garantir convergência, mas o treinamento pode ser muito lento.\n",
    "\n",
    "**Tipos**\n",
    "\n",
    "- **‘constant’**  - é constante e dado por *learning_rate_init* (default= 0.001)\n",
    "- **‘invscaling’** - diminui gradualmente o *learning rate* usando um expoente inverso a t: effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "- **‘adaptive’** - mantém o *learning rate* constante contanto que a perda de treinamento continue redunzindo\n",
    "\n",
    "### 2. Termo de Bias (`bias`):\n",
    "\n",
    "O termo de bias é um parâmetro adicional em cada camada da rede neural. Ele é adicionado aos produtos escalares dos pesos e entradas antes de passar pela função de ativação. O bias permite à rede neural aprender uma transformação mais flexível e ajustar o modelo para se adaptar melhor aos dados.\n",
    "\n",
    "### 3. Funções de Ativação:\n",
    "\n",
    "Funções de ativação introduzem não linearidades na rede neural e são essenciais para permitir que a rede aprenda relações complexas nos dados. Duas funções comuns são a **Função Sigmoide** e a **Função Tangente Hiperbólica (tanh)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base\n",
      " - entradas\n",
      " [[0 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 1 1]]  - saidas [0 0 0 0 0 0 0 1] \n",
      "\n",
      "Resultado:\n",
      " - Relu:[1]\n",
      " - Tahn:[1]\n",
      " - Sigmore:[0]\n",
      "accuracy\n",
      " - Relu:1.0\n",
      " - Tahn:1.0\n",
      " - Sigmore:0.875\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32m/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Testa o MLP com novas entradas fornecidas pelo usuário\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     user_inputs \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDigite a entrada \u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m \u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m (0 ou 1): \u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(num_inputs)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Faz uma previsão com o MLP treinado\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     predictionR \u001b[39m=\u001b[39m mlpR\u001b[39m.\u001b[39mpredict([user_inputs])\n",
      "\u001b[1;32m/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Testa o MLP com novas entradas fornecidas pelo usuário\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     user_inputs \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39;49m(\u001b[39minput\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDigite a entrada \u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m \u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m (0 ou 1): \u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_inputs)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# Faz uma previsão com o MLP treinado\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beatrizfcmenezes/Documents/Faculdade/PUC/IA/Atividades/AI/lista-extra/backpropagation.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     predictionR \u001b[39m=\u001b[39m mlpR\u001b[39m.\u001b[39mpredict([user_inputs])\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "- Com base nos experimentos foi possivel perceber que a função de ativação de Tahn levou a resultados mais exatos em até 3 entradas.\n",
    "- a partir de 4 entradas, resultados mais raros - como 1 na porta And - começaram a sererem previstos errados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
