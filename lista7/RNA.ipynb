{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bPVTxb4akHi"
      },
      "source": [
        "**Vamos experimentar agora a Rede Neural Artificial?**\n",
        "Veja:\n",
        "https://scikit-learn.org/stable/modules/neural_networks_supervised.html# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fpe0EYaXiIPm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 -q install yellowbrick\n",
        "!pip3 -q install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ru9xg6QIaceV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read and separate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "STeZ46Y4bKfl"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>menopause</th>\n",
              "      <th>tumor-size</th>\n",
              "      <th>inv-nodes</th>\n",
              "      <th>node-caps</th>\n",
              "      <th>deg-malig</th>\n",
              "      <th>breast</th>\n",
              "      <th>breast-quad</th>\n",
              "      <th>irradiat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40-49</td>\n",
              "      <td>premeno</td>\n",
              "      <td>15-19</td>\n",
              "      <td>0-2</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>right</td>\n",
              "      <td>left_up</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50-59</td>\n",
              "      <td>ge40</td>\n",
              "      <td>15-19</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>right</td>\n",
              "      <td>central</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50-59</td>\n",
              "      <td>ge40</td>\n",
              "      <td>35-39</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>left</td>\n",
              "      <td>left_low</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40-49</td>\n",
              "      <td>premeno</td>\n",
              "      <td>35-39</td>\n",
              "      <td>0-2</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>right</td>\n",
              "      <td>left_low</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40-49</td>\n",
              "      <td>premeno</td>\n",
              "      <td>30-34</td>\n",
              "      <td>3-5</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>left</td>\n",
              "      <td>right_up</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>50-59</td>\n",
              "      <td>ge40</td>\n",
              "      <td>30-34</td>\n",
              "      <td>6-8</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>left</td>\n",
              "      <td>left_low</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>50-59</td>\n",
              "      <td>premeno</td>\n",
              "      <td>25-29</td>\n",
              "      <td>3-5</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>left</td>\n",
              "      <td>left_low</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>30-39</td>\n",
              "      <td>premeno</td>\n",
              "      <td>30-34</td>\n",
              "      <td>6-8</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>right</td>\n",
              "      <td>right_up</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>50-59</td>\n",
              "      <td>premeno</td>\n",
              "      <td>15-19</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>right</td>\n",
              "      <td>left_low</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>50-59</td>\n",
              "      <td>ge40</td>\n",
              "      <td>40-44</td>\n",
              "      <td>0-2</td>\n",
              "      <td>no</td>\n",
              "      <td>3</td>\n",
              "      <td>left</td>\n",
              "      <td>right_up</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>286 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age menopause tumor-size inv-nodes node-caps  deg-malig breast  \\\n",
              "0    40-49   premeno      15-19       0-2       yes          3  right   \n",
              "1    50-59      ge40      15-19       0-2        no          1  right   \n",
              "2    50-59      ge40      35-39       0-2        no          2   left   \n",
              "3    40-49   premeno      35-39       0-2       yes          3  right   \n",
              "4    40-49   premeno      30-34       3-5       yes          2   left   \n",
              "..     ...       ...        ...       ...       ...        ...    ...   \n",
              "281  50-59      ge40      30-34       6-8       yes          2   left   \n",
              "282  50-59   premeno      25-29       3-5       yes          2   left   \n",
              "283  30-39   premeno      30-34       6-8       yes          2  right   \n",
              "284  50-59   premeno      15-19       0-2        no          2  right   \n",
              "285  50-59      ge40      40-44       0-2        no          3   left   \n",
              "\n",
              "    breast-quad irradiat  \n",
              "0       left_up       no  \n",
              "1       central       no  \n",
              "2      left_low       no  \n",
              "3      left_low      yes  \n",
              "4      right_up       no  \n",
              "..          ...      ...  \n",
              "281    left_low       no  \n",
              "282    left_low      yes  \n",
              "283    right_up       no  \n",
              "284    left_low       no  \n",
              "285    right_up       no  \n",
              "\n",
              "[286 rows x 9 columns]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"breast-cancer.csv\")\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handling outliers using Z-score\n",
        "z_score = np.abs(stats.zscore(X.select_dtypes(include=[np.number])))\n",
        "outlier_rows = np.where(z_score > 3)[0]\n",
        "X = X.drop(outlier_rows)\n",
        "y = y.drop(outlier_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess categorical and numerical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "X = preprocessor.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Oversampling using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bCxFBVNFt22"
      },
      "source": [
        "**Vamos treinar com a rede neural?**\n",
        "\n",
        "**Experimente a RNA com os parâmetros default. A rede convergiu? quantas épocas?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVW22XucaswH",
        "outputId": "2c08bca3-ed7c-4d92-c4a9-8030b70af513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.72828365\n",
            "Iteration 2, loss = 0.72588065\n",
            "Iteration 3, loss = 0.72372861\n",
            "Iteration 4, loss = 0.72154893\n",
            "Iteration 5, loss = 0.71975038\n",
            "Iteration 6, loss = 0.71796308\n",
            "Iteration 7, loss = 0.71656065\n",
            "Iteration 8, loss = 0.71481985\n",
            "Iteration 9, loss = 0.71357833\n",
            "Iteration 10, loss = 0.71245672\n",
            "Iteration 11, loss = 0.71171496\n",
            "Iteration 12, loss = 0.71096508\n",
            "Iteration 13, loss = 0.71012210\n",
            "Iteration 14, loss = 0.70953295\n",
            "Iteration 15, loss = 0.70884807\n",
            "Iteration 16, loss = 0.70826867\n",
            "Iteration 17, loss = 0.70769039\n",
            "Iteration 18, loss = 0.70710656\n",
            "Iteration 19, loss = 0.70661326\n",
            "Iteration 20, loss = 0.70606049\n",
            "Iteration 21, loss = 0.70557587\n",
            "Iteration 22, loss = 0.70498054\n",
            "Iteration 23, loss = 0.70444834\n",
            "Iteration 24, loss = 0.70394739\n",
            "Iteration 25, loss = 0.70340703\n",
            "Iteration 26, loss = 0.70291719\n",
            "Iteration 27, loss = 0.70239775\n",
            "Iteration 28, loss = 0.70188644\n",
            "Iteration 29, loss = 0.70139159\n",
            "Iteration 30, loss = 0.70086240\n",
            "Iteration 31, loss = 0.70042313\n",
            "Iteration 32, loss = 0.69997703\n",
            "Iteration 33, loss = 0.69947284\n",
            "Iteration 34, loss = 0.69900879\n",
            "Iteration 35, loss = 0.69850197\n",
            "Iteration 36, loss = 0.69804352\n",
            "Iteration 37, loss = 0.69759529\n",
            "Iteration 38, loss = 0.69710423\n",
            "Iteration 39, loss = 0.69665961\n",
            "Iteration 40, loss = 0.69616347\n",
            "Iteration 41, loss = 0.69572090\n",
            "Iteration 42, loss = 0.69524639\n",
            "Iteration 43, loss = 0.69473375\n",
            "Iteration 44, loss = 0.69424344\n",
            "Iteration 45, loss = 0.69377025\n",
            "Iteration 46, loss = 0.69325951\n",
            "Iteration 47, loss = 0.69274660\n",
            "Iteration 48, loss = 0.69230415\n",
            "Iteration 49, loss = 0.69178204\n",
            "Iteration 50, loss = 0.69128608\n",
            "Iteration 51, loss = 0.69082494\n",
            "Iteration 52, loss = 0.69028424\n",
            "Iteration 53, loss = 0.68979647\n",
            "Iteration 54, loss = 0.68928915\n",
            "Iteration 55, loss = 0.68879722\n",
            "Iteration 56, loss = 0.68826016\n",
            "Iteration 57, loss = 0.68777200\n",
            "Iteration 58, loss = 0.68724405\n",
            "Iteration 59, loss = 0.68670234\n",
            "Iteration 60, loss = 0.68617535\n",
            "Iteration 61, loss = 0.68564508\n",
            "Iteration 62, loss = 0.68509482\n",
            "Iteration 63, loss = 0.68452956\n",
            "Iteration 64, loss = 0.68398467\n",
            "Iteration 65, loss = 0.68342836\n",
            "Iteration 66, loss = 0.68291136\n",
            "Iteration 67, loss = 0.68235363\n",
            "Iteration 68, loss = 0.68176283\n",
            "Iteration 69, loss = 0.68123709\n",
            "Iteration 70, loss = 0.68066164\n",
            "Iteration 71, loss = 0.68010411\n",
            "Iteration 72, loss = 0.67946890\n",
            "Iteration 73, loss = 0.67889798\n",
            "Iteration 74, loss = 0.67831366\n",
            "Iteration 75, loss = 0.67772793\n",
            "Iteration 76, loss = 0.67718774\n",
            "Iteration 77, loss = 0.67650554\n",
            "Iteration 78, loss = 0.67593689\n",
            "Iteration 79, loss = 0.67528812\n",
            "Iteration 80, loss = 0.67470209\n",
            "Iteration 81, loss = 0.67405217\n",
            "Iteration 82, loss = 0.67346651\n",
            "Iteration 83, loss = 0.67285635\n",
            "Iteration 84, loss = 0.67226804\n",
            "Iteration 85, loss = 0.67166622\n",
            "Iteration 86, loss = 0.67105520\n",
            "Iteration 87, loss = 0.67037050\n",
            "Iteration 88, loss = 0.66969862\n",
            "Iteration 89, loss = 0.66903909\n",
            "Iteration 90, loss = 0.66828890\n",
            "Iteration 91, loss = 0.66753265\n",
            "Iteration 92, loss = 0.66670892\n",
            "Iteration 93, loss = 0.66587662\n",
            "Iteration 94, loss = 0.66494336\n",
            "Iteration 95, loss = 0.66411929\n",
            "Iteration 96, loss = 0.66317777\n",
            "Iteration 97, loss = 0.66218948\n",
            "Iteration 98, loss = 0.66136314\n",
            "Iteration 99, loss = 0.66034031\n",
            "Iteration 100, loss = 0.65940041\n",
            "Iteration 101, loss = 0.65837201\n",
            "Iteration 102, loss = 0.65743673\n",
            "Iteration 103, loss = 0.65645821\n",
            "Iteration 104, loss = 0.65537936\n",
            "Iteration 105, loss = 0.65436468\n",
            "Iteration 106, loss = 0.65355806\n",
            "Iteration 107, loss = 0.65243475\n",
            "Iteration 108, loss = 0.65151443\n",
            "Iteration 109, loss = 0.65063227\n",
            "Iteration 110, loss = 0.64976446\n",
            "Iteration 111, loss = 0.64911319\n",
            "Iteration 112, loss = 0.64817900\n",
            "Iteration 113, loss = 0.64737617\n",
            "Iteration 114, loss = 0.64664684\n",
            "Iteration 115, loss = 0.64586281\n",
            "Iteration 116, loss = 0.64509721\n",
            "Iteration 117, loss = 0.64432228\n",
            "Iteration 118, loss = 0.64360157\n",
            "Iteration 119, loss = 0.64287461\n",
            "Iteration 120, loss = 0.64213957\n",
            "Iteration 121, loss = 0.64137354\n",
            "Iteration 122, loss = 0.64063040\n",
            "Iteration 123, loss = 0.63992617\n",
            "Iteration 124, loss = 0.63913821\n",
            "Iteration 125, loss = 0.63830820\n",
            "Iteration 126, loss = 0.63755043\n",
            "Iteration 127, loss = 0.63685106\n",
            "Iteration 128, loss = 0.63609635\n",
            "Iteration 129, loss = 0.63534549\n",
            "Iteration 130, loss = 0.63465342\n",
            "Iteration 131, loss = 0.63394442\n",
            "Iteration 132, loss = 0.63324183\n",
            "Iteration 133, loss = 0.63257818\n",
            "Iteration 134, loss = 0.63184442\n",
            "Iteration 135, loss = 0.63118001\n",
            "Iteration 136, loss = 0.63055429\n",
            "Iteration 137, loss = 0.62982934\n",
            "Iteration 138, loss = 0.62919326\n",
            "Iteration 139, loss = 0.62853360\n",
            "Iteration 140, loss = 0.62791102\n",
            "Iteration 141, loss = 0.62728476\n",
            "Iteration 142, loss = 0.62668516\n",
            "Iteration 143, loss = 0.62608632\n",
            "Iteration 144, loss = 0.62542270\n",
            "Iteration 145, loss = 0.62490985\n",
            "Iteration 146, loss = 0.62428767\n",
            "Iteration 147, loss = 0.62372348\n",
            "Iteration 148, loss = 0.62314236\n",
            "Iteration 149, loss = 0.62259076\n",
            "Iteration 150, loss = 0.62205948\n",
            "Iteration 151, loss = 0.62151288\n",
            "Iteration 152, loss = 0.62096852\n",
            "Iteration 153, loss = 0.62043121\n",
            "Iteration 154, loss = 0.61984006\n",
            "Iteration 155, loss = 0.61927482\n",
            "Iteration 156, loss = 0.61871135\n",
            "Iteration 157, loss = 0.61818301\n",
            "Iteration 158, loss = 0.61757856\n",
            "Iteration 159, loss = 0.61702022\n",
            "Iteration 160, loss = 0.61643526\n",
            "Iteration 161, loss = 0.61586433\n",
            "Iteration 162, loss = 0.61528079\n",
            "Iteration 163, loss = 0.61473920\n",
            "Iteration 164, loss = 0.61418275\n",
            "Iteration 165, loss = 0.61359629\n",
            "Iteration 166, loss = 0.61306632\n",
            "Iteration 167, loss = 0.61247410\n",
            "Iteration 168, loss = 0.61187983\n",
            "Iteration 169, loss = 0.61141213\n",
            "Iteration 170, loss = 0.61085318\n",
            "Iteration 171, loss = 0.61033609\n",
            "Iteration 172, loss = 0.60977216\n",
            "Iteration 173, loss = 0.60925425\n",
            "Iteration 174, loss = 0.60868287\n",
            "Iteration 175, loss = 0.60813598\n",
            "Iteration 176, loss = 0.60763595\n",
            "Iteration 177, loss = 0.60710367\n",
            "Iteration 178, loss = 0.60653699\n",
            "Iteration 179, loss = 0.60597301\n",
            "Iteration 180, loss = 0.60542007\n",
            "Iteration 181, loss = 0.60485607\n",
            "Iteration 182, loss = 0.60435601\n",
            "Iteration 183, loss = 0.60385484\n",
            "Iteration 184, loss = 0.60329654\n",
            "Iteration 185, loss = 0.60278848\n",
            "Iteration 186, loss = 0.60218616\n",
            "Iteration 187, loss = 0.60159121\n",
            "Iteration 188, loss = 0.60107622\n",
            "Iteration 189, loss = 0.60050336\n",
            "Iteration 190, loss = 0.59993871\n",
            "Iteration 191, loss = 0.59935378\n",
            "Iteration 192, loss = 0.59877172\n",
            "Iteration 193, loss = 0.59823512\n",
            "Iteration 194, loss = 0.59762513\n",
            "Iteration 195, loss = 0.59710639\n",
            "Iteration 196, loss = 0.59650067\n",
            "Iteration 197, loss = 0.59589931\n",
            "Iteration 198, loss = 0.59537169\n",
            "Iteration 199, loss = 0.59476596\n",
            "Iteration 200, loss = 0.59417735\n",
            "Iteration 201, loss = 0.59359791\n",
            "Iteration 202, loss = 0.59299093\n",
            "Iteration 203, loss = 0.59244308\n",
            "Iteration 204, loss = 0.59180798\n",
            "Iteration 205, loss = 0.59127347\n",
            "Iteration 206, loss = 0.59065874\n",
            "Iteration 207, loss = 0.59010571\n",
            "Iteration 208, loss = 0.58952368\n",
            "Iteration 209, loss = 0.58892550\n",
            "Iteration 210, loss = 0.58840300\n",
            "Iteration 211, loss = 0.58778263\n",
            "Iteration 212, loss = 0.58719738\n",
            "Iteration 213, loss = 0.58661866\n",
            "Iteration 214, loss = 0.58615850\n",
            "Iteration 215, loss = 0.58545295\n",
            "Iteration 216, loss = 0.58480215\n",
            "Iteration 217, loss = 0.58423828\n",
            "Iteration 218, loss = 0.58360105\n",
            "Iteration 219, loss = 0.58298399\n",
            "Iteration 220, loss = 0.58246518\n",
            "Iteration 221, loss = 0.58173321\n",
            "Iteration 222, loss = 0.58123673\n",
            "Iteration 223, loss = 0.58060204\n",
            "Iteration 224, loss = 0.57999869\n",
            "Iteration 225, loss = 0.57933604\n",
            "Iteration 226, loss = 0.57890557\n",
            "Iteration 227, loss = 0.57834944\n",
            "Iteration 228, loss = 0.57771353\n",
            "Iteration 229, loss = 0.57720650\n",
            "Iteration 230, loss = 0.57664805\n",
            "Iteration 231, loss = 0.57601482\n",
            "Iteration 232, loss = 0.57548591\n",
            "Iteration 233, loss = 0.57503211\n",
            "Iteration 234, loss = 0.57445324\n",
            "Iteration 235, loss = 0.57390950\n",
            "Iteration 236, loss = 0.57341428\n",
            "Iteration 237, loss = 0.57288304\n",
            "Iteration 238, loss = 0.57233390\n",
            "Iteration 239, loss = 0.57174830\n",
            "Iteration 240, loss = 0.57119739\n",
            "Iteration 241, loss = 0.57072125\n",
            "Iteration 242, loss = 0.57019199\n",
            "Iteration 243, loss = 0.56963602\n",
            "Iteration 244, loss = 0.56908388\n",
            "Iteration 245, loss = 0.56858775\n",
            "Iteration 246, loss = 0.56807458\n",
            "Iteration 247, loss = 0.56750208\n",
            "Iteration 248, loss = 0.56700951\n",
            "Iteration 249, loss = 0.56665569\n",
            "Iteration 250, loss = 0.56595053\n",
            "Iteration 251, loss = 0.56553608\n",
            "Iteration 252, loss = 0.56505235\n",
            "Iteration 253, loss = 0.56451350\n",
            "Iteration 254, loss = 0.56399691\n",
            "Iteration 255, loss = 0.56345785\n",
            "Iteration 256, loss = 0.56290537\n",
            "Iteration 257, loss = 0.56249920\n",
            "Iteration 258, loss = 0.56190693\n",
            "Iteration 259, loss = 0.56143639\n",
            "Iteration 260, loss = 0.56103451\n",
            "Iteration 261, loss = 0.56043940\n",
            "Iteration 262, loss = 0.55991261\n",
            "Iteration 263, loss = 0.55935221\n",
            "Iteration 264, loss = 0.55889031\n",
            "Iteration 265, loss = 0.55846292\n",
            "Iteration 266, loss = 0.55791402\n",
            "Iteration 267, loss = 0.55739396\n",
            "Iteration 268, loss = 0.55691654\n",
            "Iteration 269, loss = 0.55646587\n",
            "Iteration 270, loss = 0.55597255\n",
            "Iteration 271, loss = 0.55545843\n",
            "Iteration 272, loss = 0.55490558\n",
            "Iteration 273, loss = 0.55434761\n",
            "Iteration 274, loss = 0.55372716\n",
            "Iteration 275, loss = 0.55311417\n",
            "Iteration 276, loss = 0.55280111\n",
            "Iteration 277, loss = 0.55202760\n",
            "Iteration 278, loss = 0.55144853\n",
            "Iteration 279, loss = 0.55091692\n",
            "Iteration 280, loss = 0.55034313\n",
            "Iteration 281, loss = 0.54969141\n",
            "Iteration 282, loss = 0.54909597\n",
            "Iteration 283, loss = 0.54851057\n",
            "Iteration 284, loss = 0.54788214\n",
            "Iteration 285, loss = 0.54738917\n",
            "Iteration 286, loss = 0.54668350\n",
            "Iteration 287, loss = 0.54596106\n",
            "Iteration 288, loss = 0.54527132\n",
            "Iteration 289, loss = 0.54476868\n",
            "Iteration 290, loss = 0.54415269\n",
            "Iteration 291, loss = 0.54357404\n",
            "Iteration 292, loss = 0.54302166\n",
            "Iteration 293, loss = 0.54239509\n",
            "Iteration 294, loss = 0.54178818\n",
            "Iteration 295, loss = 0.54125959\n",
            "Iteration 296, loss = 0.54071892\n",
            "Iteration 297, loss = 0.54006060\n",
            "Iteration 298, loss = 0.53952483\n",
            "Iteration 299, loss = 0.53900615\n",
            "Iteration 300, loss = 0.53845286\n",
            "Iteration 301, loss = 0.53789292\n",
            "Iteration 302, loss = 0.53734035\n",
            "Iteration 303, loss = 0.53684771\n",
            "Iteration 304, loss = 0.53638095\n",
            "Iteration 305, loss = 0.53576624\n",
            "Iteration 306, loss = 0.53529757\n",
            "Iteration 307, loss = 0.53487391\n",
            "Iteration 308, loss = 0.53439917\n",
            "Iteration 309, loss = 0.53386316\n",
            "Iteration 310, loss = 0.53331613\n",
            "Iteration 311, loss = 0.53279264\n",
            "Iteration 312, loss = 0.53225967\n",
            "Iteration 313, loss = 0.53175056\n",
            "Iteration 314, loss = 0.53120100\n",
            "Iteration 315, loss = 0.53051282\n",
            "Iteration 316, loss = 0.53002811\n",
            "Iteration 317, loss = 0.52928333\n",
            "Iteration 318, loss = 0.52865291\n",
            "Iteration 319, loss = 0.52799738\n",
            "Iteration 320, loss = 0.52740969\n",
            "Iteration 321, loss = 0.52669778\n",
            "Iteration 322, loss = 0.52626168\n",
            "Iteration 323, loss = 0.52548089\n",
            "Iteration 324, loss = 0.52481347\n",
            "Iteration 325, loss = 0.52427714\n",
            "Iteration 326, loss = 0.52368774\n",
            "Iteration 327, loss = 0.52311319\n",
            "Iteration 328, loss = 0.52259810\n",
            "Iteration 329, loss = 0.52207808\n",
            "Iteration 330, loss = 0.52149663\n",
            "Iteration 331, loss = 0.52101800\n",
            "Iteration 332, loss = 0.52051297\n",
            "Iteration 333, loss = 0.51995012\n",
            "Iteration 334, loss = 0.51953750\n",
            "Iteration 335, loss = 0.51899247\n",
            "Iteration 336, loss = 0.51845113\n",
            "Iteration 337, loss = 0.51799522\n",
            "Iteration 338, loss = 0.51753430\n",
            "Iteration 339, loss = 0.51704686\n",
            "Iteration 340, loss = 0.51652604\n",
            "Iteration 341, loss = 0.51601547\n",
            "Iteration 342, loss = 0.51543602\n",
            "Iteration 343, loss = 0.51502827\n",
            "Iteration 344, loss = 0.51457809\n",
            "Iteration 345, loss = 0.51410210\n",
            "Iteration 346, loss = 0.51363496\n",
            "Iteration 347, loss = 0.51316665\n",
            "Iteration 348, loss = 0.51265916\n",
            "Iteration 349, loss = 0.51218920\n",
            "Iteration 350, loss = 0.51169183\n",
            "Iteration 351, loss = 0.51126511\n",
            "Iteration 352, loss = 0.51081529\n",
            "Iteration 353, loss = 0.51033064\n",
            "Iteration 354, loss = 0.50982154\n",
            "Iteration 355, loss = 0.50937474\n",
            "Iteration 356, loss = 0.50877382\n",
            "Iteration 357, loss = 0.50831107\n",
            "Iteration 358, loss = 0.50780495\n",
            "Iteration 359, loss = 0.50729147\n",
            "Iteration 360, loss = 0.50685903\n",
            "Iteration 361, loss = 0.50635034\n",
            "Iteration 362, loss = 0.50596512\n",
            "Iteration 363, loss = 0.50544648\n",
            "Iteration 364, loss = 0.50493582\n",
            "Iteration 365, loss = 0.50448428\n",
            "Iteration 366, loss = 0.50397982\n",
            "Iteration 367, loss = 0.50351719\n",
            "Iteration 368, loss = 0.50304617\n",
            "Iteration 369, loss = 0.50258806\n",
            "Iteration 370, loss = 0.50220171\n",
            "Iteration 371, loss = 0.50175738\n",
            "Iteration 372, loss = 0.50122757\n",
            "Iteration 373, loss = 0.50068952\n",
            "Iteration 374, loss = 0.50031189\n",
            "Iteration 375, loss = 0.49986265\n",
            "Iteration 376, loss = 0.49940785\n",
            "Iteration 377, loss = 0.49899340\n",
            "Iteration 378, loss = 0.49846338\n",
            "Iteration 379, loss = 0.49803940\n",
            "Iteration 380, loss = 0.49753818\n",
            "Iteration 381, loss = 0.49709390\n",
            "Iteration 382, loss = 0.49661028\n",
            "Iteration 383, loss = 0.49616811\n",
            "Iteration 384, loss = 0.49574223\n",
            "Iteration 385, loss = 0.49528137\n",
            "Iteration 386, loss = 0.49494038\n",
            "Iteration 387, loss = 0.49453088\n",
            "Iteration 388, loss = 0.49407577\n",
            "Iteration 389, loss = 0.49370689\n",
            "Iteration 390, loss = 0.49326939\n",
            "Iteration 391, loss = 0.49274264\n",
            "Iteration 392, loss = 0.49247045\n",
            "Iteration 393, loss = 0.49199419\n",
            "Iteration 394, loss = 0.49159690\n",
            "Iteration 395, loss = 0.49125902\n",
            "Iteration 396, loss = 0.49061928\n",
            "Iteration 397, loss = 0.49021079\n",
            "Iteration 398, loss = 0.48975888\n",
            "Iteration 399, loss = 0.48936102\n",
            "Iteration 400, loss = 0.48892070\n",
            "Iteration 401, loss = 0.48837992\n",
            "Iteration 402, loss = 0.48778398\n",
            "Iteration 403, loss = 0.48714596\n",
            "Iteration 404, loss = 0.48645644\n",
            "Iteration 405, loss = 0.48575608\n",
            "Iteration 406, loss = 0.48495848\n",
            "Iteration 407, loss = 0.48431937\n",
            "Iteration 408, loss = 0.48383125\n",
            "Iteration 409, loss = 0.48310301\n",
            "Iteration 410, loss = 0.48249591\n",
            "Iteration 411, loss = 0.48196426\n",
            "Iteration 412, loss = 0.48130648\n",
            "Iteration 413, loss = 0.48056254\n",
            "Iteration 414, loss = 0.47982992\n",
            "Iteration 415, loss = 0.47897856\n",
            "Iteration 416, loss = 0.47827529\n",
            "Iteration 417, loss = 0.47740331\n",
            "Iteration 418, loss = 0.47669401\n",
            "Iteration 419, loss = 0.47592494\n",
            "Iteration 420, loss = 0.47517214\n",
            "Iteration 421, loss = 0.47452459\n",
            "Iteration 422, loss = 0.47400284\n",
            "Iteration 423, loss = 0.47329767\n",
            "Iteration 424, loss = 0.47273813\n",
            "Iteration 425, loss = 0.47216928\n",
            "Iteration 426, loss = 0.47160228\n",
            "Iteration 427, loss = 0.47105840\n",
            "Iteration 428, loss = 0.47053043\n",
            "Iteration 429, loss = 0.47008347\n",
            "Iteration 430, loss = 0.46952843\n",
            "Iteration 431, loss = 0.46900190\n",
            "Iteration 432, loss = 0.46853356\n",
            "Iteration 433, loss = 0.46797185\n",
            "Iteration 434, loss = 0.46744553\n",
            "Iteration 435, loss = 0.46706664\n",
            "Iteration 436, loss = 0.46651720\n",
            "Iteration 437, loss = 0.46601409\n",
            "Iteration 438, loss = 0.46559133\n",
            "Iteration 439, loss = 0.46522697\n",
            "Iteration 440, loss = 0.46464393\n",
            "Iteration 441, loss = 0.46421753\n",
            "Iteration 442, loss = 0.46385033\n",
            "Iteration 443, loss = 0.46339783\n",
            "Iteration 444, loss = 0.46281447\n",
            "Iteration 445, loss = 0.46243139\n",
            "Iteration 446, loss = 0.46213911\n",
            "Iteration 447, loss = 0.46153186\n",
            "Iteration 448, loss = 0.46117461\n",
            "Iteration 449, loss = 0.46074512\n",
            "Iteration 450, loss = 0.46036274\n",
            "Iteration 451, loss = 0.45992939\n",
            "Iteration 452, loss = 0.45948462\n",
            "Iteration 453, loss = 0.45902467\n",
            "Iteration 454, loss = 0.45859019\n",
            "Iteration 455, loss = 0.45814429\n",
            "Iteration 456, loss = 0.45781581\n",
            "Iteration 457, loss = 0.45739062\n",
            "Iteration 458, loss = 0.45702408\n",
            "Iteration 459, loss = 0.45655315\n",
            "Iteration 460, loss = 0.45616258\n",
            "Iteration 461, loss = 0.45581074\n",
            "Iteration 462, loss = 0.45535764\n",
            "Iteration 463, loss = 0.45498963\n",
            "Iteration 464, loss = 0.45458232\n",
            "Iteration 465, loss = 0.45424433\n",
            "Iteration 466, loss = 0.45384131\n",
            "Iteration 467, loss = 0.45352008\n",
            "Iteration 468, loss = 0.45313598\n",
            "Iteration 469, loss = 0.45286604\n",
            "Iteration 470, loss = 0.45236337\n",
            "Iteration 471, loss = 0.45204835\n",
            "Iteration 472, loss = 0.45180194\n",
            "Iteration 473, loss = 0.45135449\n",
            "Iteration 474, loss = 0.45099719\n",
            "Iteration 475, loss = 0.45046526\n",
            "Iteration 476, loss = 0.44997955\n",
            "Iteration 477, loss = 0.44953025\n",
            "Iteration 478, loss = 0.44890213\n",
            "Iteration 479, loss = 0.44826666\n",
            "Iteration 480, loss = 0.44768728\n",
            "Iteration 481, loss = 0.44711037\n",
            "Iteration 482, loss = 0.44660282\n",
            "Iteration 483, loss = 0.44587792\n",
            "Iteration 484, loss = 0.44551007\n",
            "Iteration 485, loss = 0.44518055\n",
            "Iteration 486, loss = 0.44455647\n",
            "Iteration 487, loss = 0.44413914\n",
            "Iteration 488, loss = 0.44358494\n",
            "Iteration 489, loss = 0.44323798\n",
            "Iteration 490, loss = 0.44263396\n",
            "Iteration 491, loss = 0.44210629\n",
            "Iteration 492, loss = 0.44162215\n",
            "Iteration 493, loss = 0.44115842\n",
            "Iteration 494, loss = 0.44049041\n",
            "Iteration 495, loss = 0.43996117\n",
            "Iteration 496, loss = 0.43960173\n",
            "Iteration 497, loss = 0.43905695\n",
            "Iteration 498, loss = 0.43856580\n",
            "Iteration 499, loss = 0.43806700\n",
            "Iteration 500, loss = 0.43754069\n",
            "Iteration 501, loss = 0.43701609\n",
            "Iteration 502, loss = 0.43658429\n",
            "Iteration 503, loss = 0.43622953\n",
            "Iteration 504, loss = 0.43573245\n",
            "Iteration 505, loss = 0.43524155\n",
            "Iteration 506, loss = 0.43497744\n",
            "Iteration 507, loss = 0.43455550\n",
            "Iteration 508, loss = 0.43400708\n",
            "Iteration 509, loss = 0.43351058\n",
            "Iteration 510, loss = 0.43301902\n",
            "Iteration 511, loss = 0.43261537\n",
            "Iteration 512, loss = 0.43245639\n",
            "Iteration 513, loss = 0.43166696\n",
            "Iteration 514, loss = 0.43154975\n",
            "Iteration 515, loss = 0.43103262\n",
            "Iteration 516, loss = 0.43044713\n",
            "Iteration 517, loss = 0.43002473\n",
            "Iteration 518, loss = 0.42970026\n",
            "Iteration 519, loss = 0.42926400\n",
            "Iteration 520, loss = 0.42883502\n",
            "Iteration 521, loss = 0.42827516\n",
            "Iteration 522, loss = 0.42822058\n",
            "Iteration 523, loss = 0.42788651\n",
            "Iteration 524, loss = 0.42728814\n",
            "Iteration 525, loss = 0.42682926\n",
            "Iteration 526, loss = 0.42657713\n",
            "Iteration 527, loss = 0.42620018\n",
            "Iteration 528, loss = 0.42563543\n",
            "Iteration 529, loss = 0.42523905\n",
            "Iteration 530, loss = 0.42494291\n",
            "Iteration 531, loss = 0.42461844\n",
            "Iteration 532, loss = 0.42404549\n",
            "Iteration 533, loss = 0.42383782\n",
            "Iteration 534, loss = 0.42351853\n",
            "Iteration 535, loss = 0.42339045\n",
            "Iteration 536, loss = 0.42279605\n",
            "Iteration 537, loss = 0.42232697\n",
            "Iteration 538, loss = 0.42208082\n",
            "Iteration 539, loss = 0.42169774\n",
            "Iteration 540, loss = 0.42138999\n",
            "Iteration 541, loss = 0.42107885\n",
            "Iteration 542, loss = 0.42052544\n",
            "Iteration 543, loss = 0.42013483\n",
            "Iteration 544, loss = 0.41985765\n",
            "Iteration 545, loss = 0.41935116\n",
            "Iteration 546, loss = 0.41896521\n",
            "Iteration 547, loss = 0.41861606\n",
            "Iteration 548, loss = 0.41829254\n",
            "Iteration 549, loss = 0.41785725\n",
            "Iteration 550, loss = 0.41735246\n",
            "Iteration 551, loss = 0.41695033\n",
            "Iteration 552, loss = 0.41663027\n",
            "Iteration 553, loss = 0.41622636\n",
            "Iteration 554, loss = 0.41579060\n",
            "Iteration 555, loss = 0.41543186\n",
            "Iteration 556, loss = 0.41489328\n",
            "Iteration 557, loss = 0.41456183\n",
            "Iteration 558, loss = 0.41431956\n",
            "Iteration 559, loss = 0.41378887\n",
            "Iteration 560, loss = 0.41338946\n",
            "Iteration 561, loss = 0.41307626\n",
            "Iteration 562, loss = 0.41255146\n",
            "Iteration 563, loss = 0.41216598\n",
            "Iteration 564, loss = 0.41189650\n",
            "Iteration 565, loss = 0.41158807\n",
            "Iteration 566, loss = 0.41117960\n",
            "Iteration 567, loss = 0.41078583\n",
            "Iteration 568, loss = 0.41049656\n",
            "Iteration 569, loss = 0.41038595\n",
            "Iteration 570, loss = 0.40990007\n",
            "Iteration 571, loss = 0.40947112\n",
            "Iteration 572, loss = 0.40920749\n",
            "Iteration 573, loss = 0.40900532\n",
            "Iteration 574, loss = 0.40837718\n",
            "Iteration 575, loss = 0.40805891\n",
            "Iteration 576, loss = 0.40782133\n",
            "Iteration 577, loss = 0.40759077\n",
            "Iteration 578, loss = 0.40720738\n",
            "Iteration 579, loss = 0.40679677\n",
            "Iteration 580, loss = 0.40647375\n",
            "Iteration 581, loss = 0.40602183\n",
            "Iteration 582, loss = 0.40578386\n",
            "Iteration 583, loss = 0.40553539\n",
            "Iteration 584, loss = 0.40517917\n",
            "Iteration 585, loss = 0.40477091\n",
            "Iteration 586, loss = 0.40444722\n",
            "Iteration 587, loss = 0.40407741\n",
            "Iteration 588, loss = 0.40397813\n",
            "Iteration 589, loss = 0.40359676\n",
            "Iteration 590, loss = 0.40319654\n",
            "Iteration 591, loss = 0.40281333\n",
            "Iteration 592, loss = 0.40256446\n",
            "Iteration 593, loss = 0.40218978\n",
            "Iteration 594, loss = 0.40184269\n",
            "Iteration 595, loss = 0.40158642\n",
            "Iteration 596, loss = 0.40122419\n",
            "Iteration 597, loss = 0.40110651\n",
            "Iteration 598, loss = 0.40060772\n",
            "Iteration 599, loss = 0.40044543\n",
            "Iteration 600, loss = 0.40005481\n",
            "Iteration 601, loss = 0.39967950\n",
            "Iteration 602, loss = 0.39935671\n",
            "Iteration 603, loss = 0.39919315\n",
            "Iteration 604, loss = 0.39903528\n",
            "Iteration 605, loss = 0.39866472\n",
            "Iteration 606, loss = 0.39823740\n",
            "Iteration 607, loss = 0.39804324\n",
            "Iteration 608, loss = 0.39771427\n",
            "Iteration 609, loss = 0.39730496\n",
            "Iteration 610, loss = 0.39700530\n",
            "Iteration 611, loss = 0.39675836\n",
            "Iteration 612, loss = 0.39642118\n",
            "Iteration 613, loss = 0.39610153\n",
            "Iteration 614, loss = 0.39590016\n",
            "Iteration 615, loss = 0.39547866\n",
            "Iteration 616, loss = 0.39526249\n",
            "Iteration 617, loss = 0.39480855\n",
            "Iteration 618, loss = 0.39456494\n",
            "Iteration 619, loss = 0.39439246\n",
            "Iteration 620, loss = 0.39400290\n",
            "Iteration 621, loss = 0.39367685\n",
            "Iteration 622, loss = 0.39334403\n",
            "Iteration 623, loss = 0.39308267\n",
            "Iteration 624, loss = 0.39273324\n",
            "Iteration 625, loss = 0.39254559\n",
            "Iteration 626, loss = 0.39236180\n",
            "Iteration 627, loss = 0.39211108\n",
            "Iteration 628, loss = 0.39165094\n",
            "Iteration 629, loss = 0.39116594\n",
            "Iteration 630, loss = 0.39111298\n",
            "Iteration 631, loss = 0.39081432\n",
            "Iteration 632, loss = 0.39023146\n",
            "Iteration 633, loss = 0.39015734\n",
            "Iteration 634, loss = 0.38982333\n",
            "Iteration 635, loss = 0.38953362\n",
            "Iteration 636, loss = 0.38909757\n",
            "Iteration 637, loss = 0.38881410\n",
            "Iteration 638, loss = 0.38855631\n",
            "Iteration 639, loss = 0.38833730\n",
            "Iteration 640, loss = 0.38792704\n",
            "Iteration 641, loss = 0.38761408\n",
            "Iteration 642, loss = 0.38719668\n",
            "Iteration 643, loss = 0.38681832\n",
            "Iteration 644, loss = 0.38672195\n",
            "Iteration 645, loss = 0.38646300\n",
            "Iteration 646, loss = 0.38601480\n",
            "Iteration 647, loss = 0.38584475\n",
            "Iteration 648, loss = 0.38569984\n",
            "Iteration 649, loss = 0.38524737\n",
            "Iteration 650, loss = 0.38495738\n",
            "Iteration 651, loss = 0.38464194\n",
            "Iteration 652, loss = 0.38424663\n",
            "Iteration 653, loss = 0.38384348\n",
            "Iteration 654, loss = 0.38396367\n",
            "Iteration 655, loss = 0.38345820\n",
            "Iteration 656, loss = 0.38302294\n",
            "Iteration 657, loss = 0.38292125\n",
            "Iteration 658, loss = 0.38269713\n",
            "Iteration 659, loss = 0.38232264\n",
            "Iteration 660, loss = 0.38188730\n",
            "Iteration 661, loss = 0.38157791\n",
            "Iteration 662, loss = 0.38141375\n",
            "Iteration 663, loss = 0.38123102\n",
            "Iteration 664, loss = 0.38087151\n",
            "Iteration 665, loss = 0.38052021\n",
            "Iteration 666, loss = 0.38022320\n",
            "Iteration 667, loss = 0.37993477\n",
            "Iteration 668, loss = 0.37964459\n",
            "Iteration 669, loss = 0.37934432\n",
            "Iteration 670, loss = 0.37932064\n",
            "Iteration 671, loss = 0.37890586\n",
            "Iteration 672, loss = 0.37846267\n",
            "Iteration 673, loss = 0.37816463\n",
            "Iteration 674, loss = 0.37791108\n",
            "Iteration 675, loss = 0.37763050\n",
            "Iteration 676, loss = 0.37727802\n",
            "Iteration 677, loss = 0.37705388\n",
            "Iteration 678, loss = 0.37670516\n",
            "Iteration 679, loss = 0.37640282\n",
            "Iteration 680, loss = 0.37612959\n",
            "Iteration 681, loss = 0.37591983\n",
            "Iteration 682, loss = 0.37581498\n",
            "Iteration 683, loss = 0.37527345\n",
            "Iteration 684, loss = 0.37516122\n",
            "Iteration 685, loss = 0.37478664\n",
            "Iteration 686, loss = 0.37450781\n",
            "Iteration 687, loss = 0.37426456\n",
            "Iteration 688, loss = 0.37399931\n",
            "Iteration 689, loss = 0.37380318\n",
            "Iteration 690, loss = 0.37348901\n",
            "Iteration 691, loss = 0.37317294\n",
            "Iteration 692, loss = 0.37287867\n",
            "Iteration 693, loss = 0.37262780\n",
            "Iteration 694, loss = 0.37232498\n",
            "Iteration 695, loss = 0.37216392\n",
            "Iteration 696, loss = 0.37181480\n",
            "Iteration 697, loss = 0.37162001\n",
            "Iteration 698, loss = 0.37121145\n",
            "Iteration 699, loss = 0.37100490\n",
            "Iteration 700, loss = 0.37091114\n",
            "Iteration 701, loss = 0.37049377\n",
            "Iteration 702, loss = 0.37016476\n",
            "Iteration 703, loss = 0.36996623\n",
            "Iteration 704, loss = 0.36980190\n",
            "Iteration 705, loss = 0.36948145\n",
            "Iteration 706, loss = 0.36914295\n",
            "Iteration 707, loss = 0.36894817\n",
            "Iteration 708, loss = 0.36882041\n",
            "Iteration 709, loss = 0.36841354\n",
            "Iteration 710, loss = 0.36814754\n",
            "Iteration 711, loss = 0.36810408\n",
            "Iteration 712, loss = 0.36761353\n",
            "Iteration 713, loss = 0.36734970\n",
            "Iteration 714, loss = 0.36726777\n",
            "Iteration 715, loss = 0.36698334\n",
            "Iteration 716, loss = 0.36671327\n",
            "Iteration 717, loss = 0.36633359\n",
            "Iteration 718, loss = 0.36626525\n",
            "Iteration 719, loss = 0.36603146\n",
            "Iteration 720, loss = 0.36578154\n",
            "Iteration 721, loss = 0.36539189\n",
            "Iteration 722, loss = 0.36518558\n",
            "Iteration 723, loss = 0.36496601\n",
            "Iteration 724, loss = 0.36467489\n",
            "Iteration 725, loss = 0.36442966\n",
            "Iteration 726, loss = 0.36418266\n",
            "Iteration 727, loss = 0.36387513\n",
            "Iteration 728, loss = 0.36378061\n",
            "Iteration 729, loss = 0.36338298\n",
            "Iteration 730, loss = 0.36338471\n",
            "Iteration 731, loss = 0.36308622\n",
            "Iteration 732, loss = 0.36270885\n",
            "Iteration 733, loss = 0.36248508\n",
            "Iteration 734, loss = 0.36231914\n",
            "Iteration 735, loss = 0.36218455\n",
            "Iteration 736, loss = 0.36195383\n",
            "Iteration 737, loss = 0.36157293\n",
            "Iteration 738, loss = 0.36141429\n",
            "Iteration 739, loss = 0.36115062\n",
            "Iteration 740, loss = 0.36078299\n",
            "Iteration 741, loss = 0.36056103\n",
            "Iteration 742, loss = 0.36030495\n",
            "Iteration 743, loss = 0.36016281\n",
            "Iteration 744, loss = 0.35980421\n",
            "Iteration 745, loss = 0.35952328\n",
            "Iteration 746, loss = 0.35963547\n",
            "Iteration 747, loss = 0.35936038\n",
            "Iteration 748, loss = 0.35922722\n",
            "Iteration 749, loss = 0.35900023\n",
            "Iteration 750, loss = 0.35877354\n",
            "Iteration 751, loss = 0.35856602\n",
            "Iteration 752, loss = 0.35831807\n",
            "Iteration 753, loss = 0.35801222\n",
            "Iteration 754, loss = 0.35792311\n",
            "Iteration 755, loss = 0.35755756\n",
            "Iteration 756, loss = 0.35718128\n",
            "Iteration 757, loss = 0.35684754\n",
            "Iteration 758, loss = 0.35685830\n",
            "Iteration 759, loss = 0.35659001\n",
            "Iteration 760, loss = 0.35633486\n",
            "Iteration 761, loss = 0.35623448\n",
            "Iteration 762, loss = 0.35592943\n",
            "Iteration 763, loss = 0.35566674\n",
            "Iteration 764, loss = 0.35545158\n",
            "Iteration 765, loss = 0.35515749\n",
            "Iteration 766, loss = 0.35493303\n",
            "Iteration 767, loss = 0.35481088\n",
            "Iteration 768, loss = 0.35440243\n",
            "Iteration 769, loss = 0.35430072\n",
            "Iteration 770, loss = 0.35408220\n",
            "Iteration 771, loss = 0.35397611\n",
            "Iteration 772, loss = 0.35390663\n",
            "Iteration 773, loss = 0.35340316\n",
            "Iteration 774, loss = 0.35332500\n",
            "Iteration 775, loss = 0.35317096\n",
            "Iteration 776, loss = 0.35281885\n",
            "Iteration 777, loss = 0.35260038\n",
            "Iteration 778, loss = 0.35240969\n",
            "Iteration 779, loss = 0.35219812\n",
            "Iteration 780, loss = 0.35198535\n",
            "Iteration 781, loss = 0.35170388\n",
            "Iteration 782, loss = 0.35161918\n",
            "Iteration 783, loss = 0.35137809\n",
            "Iteration 784, loss = 0.35126183\n",
            "Iteration 785, loss = 0.35113980\n",
            "Iteration 786, loss = 0.35082802\n",
            "Iteration 787, loss = 0.35059181\n",
            "Iteration 788, loss = 0.35033646\n",
            "Iteration 789, loss = 0.35016743\n",
            "Iteration 790, loss = 0.34999993\n",
            "Iteration 791, loss = 0.34974205\n",
            "Iteration 792, loss = 0.34957990\n",
            "Iteration 793, loss = 0.34947469\n",
            "Iteration 794, loss = 0.34914481\n",
            "Iteration 795, loss = 0.34897366\n",
            "Iteration 796, loss = 0.34873268\n",
            "Iteration 797, loss = 0.34842397\n",
            "Iteration 798, loss = 0.34820319\n",
            "Iteration 799, loss = 0.34820262\n",
            "Iteration 800, loss = 0.34793116\n",
            "Iteration 801, loss = 0.34769337\n",
            "Iteration 802, loss = 0.34750531\n",
            "Iteration 803, loss = 0.34736007\n",
            "Iteration 804, loss = 0.34712750\n",
            "Iteration 805, loss = 0.34689160\n",
            "Iteration 806, loss = 0.34677361\n",
            "Iteration 807, loss = 0.34661231\n",
            "Iteration 808, loss = 0.34646307\n",
            "Iteration 809, loss = 0.34627868\n",
            "Iteration 810, loss = 0.34597541\n",
            "Iteration 811, loss = 0.34585291\n",
            "Iteration 812, loss = 0.34565371\n",
            "Iteration 813, loss = 0.34548307\n",
            "Iteration 814, loss = 0.34533692\n",
            "Iteration 815, loss = 0.34530190\n",
            "Iteration 816, loss = 0.34486553\n",
            "Iteration 817, loss = 0.34495767\n",
            "Iteration 818, loss = 0.34461638\n",
            "Iteration 819, loss = 0.34438880\n",
            "Iteration 820, loss = 0.34430970\n",
            "Iteration 821, loss = 0.34393401\n",
            "Iteration 822, loss = 0.34378488\n",
            "Iteration 823, loss = 0.34371285\n",
            "Iteration 824, loss = 0.34352587\n",
            "Iteration 825, loss = 0.34327935\n",
            "Iteration 826, loss = 0.34327126\n",
            "Iteration 827, loss = 0.34285223\n",
            "Iteration 828, loss = 0.34290925\n",
            "Iteration 829, loss = 0.34275181\n",
            "Iteration 830, loss = 0.34251010\n",
            "Iteration 831, loss = 0.34260002\n",
            "Iteration 832, loss = 0.34211937\n",
            "Iteration 833, loss = 0.34194214\n",
            "Iteration 834, loss = 0.34161841\n",
            "Iteration 835, loss = 0.34156649\n",
            "Iteration 836, loss = 0.34152363\n",
            "Iteration 837, loss = 0.34113631\n",
            "Iteration 838, loss = 0.34105277\n",
            "Iteration 839, loss = 0.34071778\n",
            "Iteration 840, loss = 0.34067370\n",
            "Iteration 841, loss = 0.34040729\n",
            "Iteration 842, loss = 0.34033911\n",
            "Iteration 843, loss = 0.34028730\n",
            "Iteration 844, loss = 0.34002377\n",
            "Iteration 845, loss = 0.33974641\n",
            "Iteration 846, loss = 0.33960866\n",
            "Iteration 847, loss = 0.33943434\n",
            "Iteration 848, loss = 0.33924161\n",
            "Iteration 849, loss = 0.33908992\n",
            "Iteration 850, loss = 0.33885573\n",
            "Iteration 851, loss = 0.33878240\n",
            "Iteration 852, loss = 0.33867174\n",
            "Iteration 853, loss = 0.33851843\n",
            "Iteration 854, loss = 0.33827331\n",
            "Iteration 855, loss = 0.33818712\n",
            "Iteration 856, loss = 0.33796045\n",
            "Iteration 857, loss = 0.33771627\n",
            "Iteration 858, loss = 0.33757578\n",
            "Iteration 859, loss = 0.33731758\n",
            "Iteration 860, loss = 0.33723250\n",
            "Iteration 861, loss = 0.33702929\n",
            "Iteration 862, loss = 0.33693366\n",
            "Iteration 863, loss = 0.33670008\n",
            "Iteration 864, loss = 0.33654706\n",
            "Iteration 865, loss = 0.33645062\n",
            "Iteration 866, loss = 0.33646394\n",
            "Iteration 867, loss = 0.33606443\n",
            "Iteration 868, loss = 0.33609420\n",
            "Iteration 869, loss = 0.33583658\n",
            "Iteration 870, loss = 0.33554234\n",
            "Iteration 871, loss = 0.33567127\n",
            "Iteration 872, loss = 0.33535644\n",
            "Iteration 873, loss = 0.33511431\n",
            "Iteration 874, loss = 0.33535359\n",
            "Iteration 875, loss = 0.33497098\n",
            "Iteration 876, loss = 0.33496898\n",
            "Iteration 877, loss = 0.33465639\n",
            "Iteration 878, loss = 0.33455333\n",
            "Iteration 879, loss = 0.33433523\n",
            "Iteration 880, loss = 0.33416708\n",
            "Iteration 881, loss = 0.33408892\n",
            "Iteration 882, loss = 0.33396692\n",
            "Iteration 883, loss = 0.33368797\n",
            "Iteration 884, loss = 0.33362230\n",
            "Iteration 885, loss = 0.33340838\n",
            "Iteration 886, loss = 0.33325945\n",
            "Iteration 887, loss = 0.33300949\n",
            "Iteration 888, loss = 0.33297421\n",
            "Iteration 889, loss = 0.33285388\n",
            "Iteration 890, loss = 0.33270271\n",
            "Iteration 891, loss = 0.33270904\n",
            "Iteration 892, loss = 0.33237038\n",
            "Iteration 893, loss = 0.33227570\n",
            "Iteration 894, loss = 0.33227256\n",
            "Iteration 895, loss = 0.33204941\n",
            "Iteration 896, loss = 0.33175859\n",
            "Iteration 897, loss = 0.33189263\n",
            "Iteration 898, loss = 0.33178291\n",
            "Iteration 899, loss = 0.33171083\n",
            "Iteration 900, loss = 0.33141809\n",
            "Iteration 901, loss = 0.33128831\n",
            "Iteration 902, loss = 0.33096854\n",
            "Iteration 903, loss = 0.33077154\n",
            "Iteration 904, loss = 0.33073074\n",
            "Iteration 905, loss = 0.33043239\n",
            "Iteration 906, loss = 0.33041302\n",
            "Iteration 907, loss = 0.33042923\n",
            "Iteration 908, loss = 0.33014389\n",
            "Iteration 909, loss = 0.32991503\n",
            "Iteration 910, loss = 0.32985005\n",
            "Iteration 911, loss = 0.32973698\n",
            "Iteration 912, loss = 0.32961452\n",
            "Iteration 913, loss = 0.32949463\n",
            "Iteration 914, loss = 0.32919217\n",
            "Iteration 915, loss = 0.32922352\n",
            "Iteration 916, loss = 0.32903122\n",
            "Iteration 917, loss = 0.32875147\n",
            "Iteration 918, loss = 0.32878818\n",
            "Iteration 919, loss = 0.32865854\n",
            "Iteration 920, loss = 0.32840945\n",
            "Iteration 921, loss = 0.32827463\n",
            "Iteration 922, loss = 0.32804904\n",
            "Iteration 923, loss = 0.32798296\n",
            "Iteration 924, loss = 0.32793503\n",
            "Iteration 925, loss = 0.32774456\n",
            "Iteration 926, loss = 0.32756852\n",
            "Iteration 927, loss = 0.32743839\n",
            "Iteration 928, loss = 0.32728699\n",
            "Iteration 929, loss = 0.32720046\n",
            "Iteration 930, loss = 0.32699377\n",
            "Iteration 931, loss = 0.32691479\n",
            "Iteration 932, loss = 0.32662368\n",
            "Iteration 933, loss = 0.32642264\n",
            "Iteration 934, loss = 0.32645399\n",
            "Iteration 935, loss = 0.32618182\n",
            "Iteration 936, loss = 0.32612494\n",
            "Iteration 937, loss = 0.32580320\n",
            "Iteration 938, loss = 0.32568503\n",
            "Iteration 939, loss = 0.32555748\n",
            "Iteration 940, loss = 0.32547325\n",
            "Iteration 941, loss = 0.32520295\n",
            "Iteration 942, loss = 0.32524011\n",
            "Iteration 943, loss = 0.32497948\n",
            "Iteration 944, loss = 0.32487382\n",
            "Iteration 945, loss = 0.32485297\n",
            "Iteration 946, loss = 0.32488074\n",
            "Iteration 947, loss = 0.32450235\n",
            "Iteration 948, loss = 0.32440490\n",
            "Iteration 949, loss = 0.32417260\n",
            "Iteration 950, loss = 0.32404850\n",
            "Iteration 951, loss = 0.32387943\n",
            "Iteration 952, loss = 0.32374857\n",
            "Iteration 953, loss = 0.32366065\n",
            "Iteration 954, loss = 0.32356295\n",
            "Iteration 955, loss = 0.32341370\n",
            "Iteration 956, loss = 0.32324905\n",
            "Iteration 957, loss = 0.32302883\n",
            "Iteration 958, loss = 0.32311699\n",
            "Iteration 959, loss = 0.32286623\n",
            "Iteration 960, loss = 0.32262105\n",
            "Iteration 961, loss = 0.32252795\n",
            "Iteration 962, loss = 0.32242990\n",
            "Iteration 963, loss = 0.32235057\n",
            "Iteration 964, loss = 0.32219734\n",
            "Iteration 965, loss = 0.32202724\n",
            "Iteration 966, loss = 0.32196714\n",
            "Iteration 967, loss = 0.32177673\n",
            "Iteration 968, loss = 0.32171666\n",
            "Iteration 969, loss = 0.32156737\n",
            "Iteration 970, loss = 0.32136146\n",
            "Iteration 971, loss = 0.32132136\n",
            "Iteration 972, loss = 0.32110624\n",
            "Iteration 973, loss = 0.32095553\n",
            "Iteration 974, loss = 0.32088868\n",
            "Iteration 975, loss = 0.32107682\n",
            "Iteration 976, loss = 0.32067145\n",
            "Iteration 977, loss = 0.32078323\n",
            "Iteration 978, loss = 0.32042482\n",
            "Iteration 979, loss = 0.32041487\n",
            "Iteration 980, loss = 0.32027395\n",
            "Iteration 981, loss = 0.32015878\n",
            "Iteration 982, loss = 0.32002610\n",
            "Iteration 983, loss = 0.31989027\n",
            "Iteration 984, loss = 0.31984121\n",
            "Iteration 985, loss = 0.31968157\n",
            "Iteration 986, loss = 0.31953431\n",
            "Iteration 987, loss = 0.31956706\n",
            "Iteration 988, loss = 0.31939997\n",
            "Iteration 989, loss = 0.31920234\n",
            "Iteration 990, loss = 0.31958182\n",
            "Iteration 991, loss = 0.31891880\n",
            "Iteration 992, loss = 0.31895265\n",
            "Iteration 993, loss = 0.31885066\n",
            "Iteration 994, loss = 0.31880360\n",
            "Iteration 995, loss = 0.31858118\n",
            "Iteration 996, loss = 0.31862953\n",
            "Iteration 997, loss = 0.31825030\n",
            "Iteration 998, loss = 0.31813022\n",
            "Iteration 999, loss = 0.31800994\n",
            "Iteration 1000, loss = 0.31790921\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(3, 2, 1), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(3, 2, 1), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(3, 2, 1), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#modelo = MLPClassifier()\n",
        "modelo = MLPClassifier(max_iter=1000, verbose=True, tol=0.00001, solver = 'adam', activation = 'relu',hidden_layer_sizes = (3, 2, 1))\n",
        "modelo.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Yh14LUHfN-"
      },
      "source": [
        "**Depois execute novamente com os ajustes. Veja agora os erros a cada época.. estabeleça o verbose para true **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QRptikHHepQ",
        "outputId": "f0106c94-1301-44e9-be60-e108bf82bebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.72818496\n",
            "Iteration 2, loss = 0.69598631\n",
            "Iteration 3, loss = 0.66825764\n",
            "Iteration 4, loss = 0.64539323\n",
            "Iteration 5, loss = 0.62597860\n",
            "Iteration 6, loss = 0.60953516\n",
            "Iteration 7, loss = 0.59574679\n",
            "Iteration 8, loss = 0.58481498\n",
            "Iteration 9, loss = 0.57623941\n",
            "Iteration 10, loss = 0.56800979\n",
            "Iteration 11, loss = 0.56198147\n",
            "Iteration 12, loss = 0.55671190\n",
            "Iteration 13, loss = 0.55231047\n",
            "Iteration 14, loss = 0.54817643\n",
            "Iteration 15, loss = 0.54441508\n",
            "Iteration 16, loss = 0.54039756\n",
            "Iteration 17, loss = 0.53656374\n",
            "Iteration 18, loss = 0.53268038\n",
            "Iteration 19, loss = 0.52909724\n",
            "Iteration 20, loss = 0.52533059\n",
            "Iteration 21, loss = 0.52215972\n",
            "Iteration 22, loss = 0.51884408\n",
            "Iteration 23, loss = 0.51551125\n",
            "Iteration 24, loss = 0.51231253\n",
            "Iteration 25, loss = 0.50909862\n",
            "Iteration 26, loss = 0.50587870\n",
            "Iteration 27, loss = 0.50268818\n",
            "Iteration 28, loss = 0.50018347\n",
            "Iteration 29, loss = 0.49705312\n",
            "Iteration 30, loss = 0.49448681\n",
            "Iteration 31, loss = 0.49221054\n",
            "Iteration 32, loss = 0.48981345\n",
            "Iteration 33, loss = 0.48755832\n",
            "Iteration 34, loss = 0.48519736\n",
            "Iteration 35, loss = 0.48294261\n",
            "Iteration 36, loss = 0.48058862\n",
            "Iteration 37, loss = 0.47838069\n",
            "Iteration 38, loss = 0.47641858\n",
            "Iteration 39, loss = 0.47435923\n",
            "Iteration 40, loss = 0.47303542\n",
            "Iteration 41, loss = 0.47120693\n",
            "Iteration 42, loss = 0.46964666\n",
            "Iteration 43, loss = 0.46822394\n",
            "Iteration 44, loss = 0.46639549\n",
            "Iteration 45, loss = 0.46487367\n",
            "Iteration 46, loss = 0.46311727\n",
            "Iteration 47, loss = 0.46202140\n",
            "Iteration 48, loss = 0.46102605\n",
            "Iteration 49, loss = 0.45983083\n",
            "Iteration 50, loss = 0.45858403\n",
            "Iteration 51, loss = 0.45742327\n",
            "Iteration 52, loss = 0.45570932\n",
            "Iteration 53, loss = 0.45381773\n",
            "Iteration 54, loss = 0.45202011\n",
            "Iteration 55, loss = 0.44997385\n",
            "Iteration 56, loss = 0.44852930\n",
            "Iteration 57, loss = 0.44666369\n",
            "Iteration 58, loss = 0.44513026\n",
            "Iteration 59, loss = 0.44353055\n",
            "Iteration 60, loss = 0.44226860\n",
            "Iteration 61, loss = 0.44081801\n",
            "Iteration 62, loss = 0.43968646\n",
            "Iteration 63, loss = 0.43823902\n",
            "Iteration 64, loss = 0.43689048\n",
            "Iteration 65, loss = 0.43546994\n",
            "Iteration 66, loss = 0.43423772\n",
            "Iteration 67, loss = 0.43292339\n",
            "Iteration 68, loss = 0.43159815\n",
            "Iteration 69, loss = 0.43019062\n",
            "Iteration 70, loss = 0.42883095\n",
            "Iteration 71, loss = 0.42766181\n",
            "Iteration 72, loss = 0.42655429\n",
            "Iteration 73, loss = 0.42527256\n",
            "Iteration 74, loss = 0.42387424\n",
            "Iteration 75, loss = 0.42246424\n",
            "Iteration 76, loss = 0.42102959\n",
            "Iteration 77, loss = 0.41974950\n",
            "Iteration 78, loss = 0.41852206\n",
            "Iteration 79, loss = 0.41721290\n",
            "Iteration 80, loss = 0.41591530\n",
            "Iteration 81, loss = 0.41460684\n",
            "Iteration 82, loss = 0.41329548\n",
            "Iteration 83, loss = 0.41208382\n",
            "Iteration 84, loss = 0.41077955\n",
            "Iteration 85, loss = 0.40955113\n",
            "Iteration 86, loss = 0.40830093\n",
            "Iteration 87, loss = 0.40709920\n",
            "Iteration 88, loss = 0.40586555\n",
            "Iteration 89, loss = 0.40456224\n",
            "Iteration 90, loss = 0.40324187\n",
            "Iteration 91, loss = 0.40176943\n",
            "Iteration 92, loss = 0.40037451\n",
            "Iteration 93, loss = 0.39886350\n",
            "Iteration 94, loss = 0.39764441\n",
            "Iteration 95, loss = 0.39649722\n",
            "Iteration 96, loss = 0.39547714\n",
            "Iteration 97, loss = 0.39419092\n",
            "Iteration 98, loss = 0.39290241\n",
            "Iteration 99, loss = 0.39162306\n",
            "Iteration 100, loss = 0.39067330\n",
            "Iteration 101, loss = 0.38931125\n",
            "Iteration 102, loss = 0.38784867\n",
            "Iteration 103, loss = 0.38647444\n",
            "Iteration 104, loss = 0.38488643\n",
            "Iteration 105, loss = 0.38365249\n",
            "Iteration 106, loss = 0.38271579\n",
            "Iteration 107, loss = 0.38174732\n",
            "Iteration 108, loss = 0.38039816\n",
            "Iteration 109, loss = 0.37895280\n",
            "Iteration 110, loss = 0.37739644\n",
            "Iteration 111, loss = 0.37589848\n",
            "Iteration 112, loss = 0.37454104\n",
            "Iteration 113, loss = 0.37327269\n",
            "Iteration 114, loss = 0.37182456\n",
            "Iteration 115, loss = 0.37055837\n",
            "Iteration 116, loss = 0.36945851\n",
            "Iteration 117, loss = 0.36824700\n",
            "Iteration 118, loss = 0.36677075\n",
            "Iteration 119, loss = 0.36541532\n",
            "Iteration 120, loss = 0.36411716\n",
            "Iteration 121, loss = 0.36280556\n",
            "Iteration 122, loss = 0.36184741\n",
            "Iteration 123, loss = 0.36033588\n",
            "Iteration 124, loss = 0.35909444\n",
            "Iteration 125, loss = 0.35766109\n",
            "Iteration 126, loss = 0.35666216\n",
            "Iteration 127, loss = 0.35522733\n",
            "Iteration 128, loss = 0.35419154\n",
            "Iteration 129, loss = 0.35289610\n",
            "Iteration 130, loss = 0.35169062\n",
            "Iteration 131, loss = 0.35063556\n",
            "Iteration 132, loss = 0.34966123\n",
            "Iteration 133, loss = 0.34857702\n",
            "Iteration 134, loss = 0.34743588\n",
            "Iteration 135, loss = 0.34632621\n",
            "Iteration 136, loss = 0.34519245\n",
            "Iteration 137, loss = 0.34380917\n",
            "Iteration 138, loss = 0.34227218\n",
            "Iteration 139, loss = 0.34095878\n",
            "Iteration 140, loss = 0.33948183\n",
            "Iteration 141, loss = 0.33819012\n",
            "Iteration 142, loss = 0.33665632\n",
            "Iteration 143, loss = 0.33569960\n",
            "Iteration 144, loss = 0.33437375\n",
            "Iteration 145, loss = 0.33322321\n",
            "Iteration 146, loss = 0.33207726\n",
            "Iteration 147, loss = 0.33089849\n",
            "Iteration 148, loss = 0.33001390\n",
            "Iteration 149, loss = 0.32904644\n",
            "Iteration 150, loss = 0.32804475\n",
            "Iteration 151, loss = 0.32669803\n",
            "Iteration 152, loss = 0.32526058\n",
            "Iteration 153, loss = 0.32390644\n",
            "Iteration 154, loss = 0.32250597\n",
            "Iteration 155, loss = 0.32162343\n",
            "Iteration 156, loss = 0.32091202\n",
            "Iteration 157, loss = 0.31978472\n",
            "Iteration 158, loss = 0.31838106\n",
            "Iteration 159, loss = 0.31713661\n",
            "Iteration 160, loss = 0.31558437\n",
            "Iteration 161, loss = 0.31424745\n",
            "Iteration 162, loss = 0.31292619\n",
            "Iteration 163, loss = 0.31173893\n",
            "Iteration 164, loss = 0.31063782\n",
            "Iteration 165, loss = 0.30964063\n",
            "Iteration 166, loss = 0.30830167\n",
            "Iteration 167, loss = 0.30704766\n",
            "Iteration 168, loss = 0.30549599\n",
            "Iteration 169, loss = 0.30417018\n",
            "Iteration 170, loss = 0.30309655\n",
            "Iteration 171, loss = 0.30167642\n",
            "Iteration 172, loss = 0.30037906\n",
            "Iteration 173, loss = 0.29901573\n",
            "Iteration 174, loss = 0.29759745\n",
            "Iteration 175, loss = 0.29680958\n",
            "Iteration 176, loss = 0.29578081\n",
            "Iteration 177, loss = 0.29487615\n",
            "Iteration 178, loss = 0.29391185\n",
            "Iteration 179, loss = 0.29319064\n",
            "Iteration 180, loss = 0.29190226\n",
            "Iteration 181, loss = 0.29046201\n",
            "Iteration 182, loss = 0.28926097\n",
            "Iteration 183, loss = 0.28826380\n",
            "Iteration 184, loss = 0.28718516\n",
            "Iteration 185, loss = 0.28611990\n",
            "Iteration 186, loss = 0.28499796\n",
            "Iteration 187, loss = 0.28372349\n",
            "Iteration 188, loss = 0.28289331\n",
            "Iteration 189, loss = 0.28170884\n",
            "Iteration 190, loss = 0.28034841\n",
            "Iteration 191, loss = 0.27922650\n",
            "Iteration 192, loss = 0.27820068\n",
            "Iteration 193, loss = 0.27731652\n",
            "Iteration 194, loss = 0.27646285\n",
            "Iteration 195, loss = 0.27550596\n",
            "Iteration 196, loss = 0.27452537\n",
            "Iteration 197, loss = 0.27354364\n",
            "Iteration 198, loss = 0.27263913\n",
            "Iteration 199, loss = 0.27157911\n",
            "Iteration 200, loss = 0.27058968\n",
            "Iteration 201, loss = 0.26941491\n",
            "Iteration 202, loss = 0.26817209\n",
            "Iteration 203, loss = 0.26750453\n",
            "Iteration 204, loss = 0.26746049\n",
            "Iteration 205, loss = 0.26646544\n",
            "Iteration 206, loss = 0.26535399\n",
            "Iteration 207, loss = 0.26401622\n",
            "Iteration 208, loss = 0.26284004\n",
            "Iteration 209, loss = 0.26155655\n",
            "Iteration 210, loss = 0.26063882\n",
            "Iteration 211, loss = 0.25976254\n",
            "Iteration 212, loss = 0.25895820\n",
            "Iteration 213, loss = 0.25808448\n",
            "Iteration 214, loss = 0.25744813\n",
            "Iteration 215, loss = 0.25667301\n",
            "Iteration 216, loss = 0.25605832\n",
            "Iteration 217, loss = 0.25505597\n",
            "Iteration 218, loss = 0.25384519\n",
            "Iteration 219, loss = 0.25258576\n",
            "Iteration 220, loss = 0.25137510\n",
            "Iteration 221, loss = 0.25023288\n",
            "Iteration 222, loss = 0.24923263\n",
            "Iteration 223, loss = 0.24815835\n",
            "Iteration 224, loss = 0.24714575\n",
            "Iteration 225, loss = 0.24621982\n",
            "Iteration 226, loss = 0.24519726\n",
            "Iteration 227, loss = 0.24425276\n",
            "Iteration 228, loss = 0.24327651\n",
            "Iteration 229, loss = 0.24226524\n",
            "Iteration 230, loss = 0.24118581\n",
            "Iteration 231, loss = 0.24025651\n",
            "Iteration 232, loss = 0.23985096\n",
            "Iteration 233, loss = 0.24017626\n",
            "Iteration 234, loss = 0.23993245\n",
            "Iteration 235, loss = 0.23914638\n",
            "Iteration 236, loss = 0.23797865\n",
            "Iteration 237, loss = 0.23658034\n",
            "Iteration 238, loss = 0.23514183\n",
            "Iteration 239, loss = 0.23372449\n",
            "Iteration 240, loss = 0.23262574\n",
            "Iteration 241, loss = 0.23228424\n",
            "Iteration 242, loss = 0.23165407\n",
            "Iteration 243, loss = 0.23089105\n",
            "Iteration 244, loss = 0.23006043\n",
            "Iteration 245, loss = 0.22901491\n",
            "Iteration 246, loss = 0.22817379\n",
            "Iteration 247, loss = 0.22752613\n",
            "Iteration 248, loss = 0.22700729\n",
            "Iteration 249, loss = 0.22653001\n",
            "Iteration 250, loss = 0.22563937\n",
            "Iteration 251, loss = 0.22455317\n",
            "Iteration 252, loss = 0.22335180\n",
            "Iteration 253, loss = 0.22247480\n",
            "Iteration 254, loss = 0.22158197\n",
            "Iteration 255, loss = 0.22067488\n",
            "Iteration 256, loss = 0.21979754\n",
            "Iteration 257, loss = 0.21916193\n",
            "Iteration 258, loss = 0.21835588\n",
            "Iteration 259, loss = 0.21770104\n",
            "Iteration 260, loss = 0.21677495\n",
            "Iteration 261, loss = 0.21599701\n",
            "Iteration 262, loss = 0.21519940\n",
            "Iteration 263, loss = 0.21443131\n",
            "Iteration 264, loss = 0.21371627\n",
            "Iteration 265, loss = 0.21305328\n",
            "Iteration 266, loss = 0.21237561\n",
            "Iteration 267, loss = 0.21194720\n",
            "Iteration 268, loss = 0.21130591\n",
            "Iteration 269, loss = 0.21046621\n",
            "Iteration 270, loss = 0.20937999\n",
            "Iteration 271, loss = 0.20807679\n",
            "Iteration 272, loss = 0.20721823\n",
            "Iteration 273, loss = 0.20648952\n",
            "Iteration 274, loss = 0.20612377\n",
            "Iteration 275, loss = 0.20585092\n",
            "Iteration 276, loss = 0.20544233\n",
            "Iteration 277, loss = 0.20456726\n",
            "Iteration 278, loss = 0.20324482\n",
            "Iteration 279, loss = 0.20235740\n",
            "Iteration 280, loss = 0.20248586\n",
            "Iteration 281, loss = 0.20164741\n",
            "Iteration 282, loss = 0.20045628\n",
            "Iteration 283, loss = 0.19942275\n",
            "Iteration 284, loss = 0.19864538\n",
            "Iteration 285, loss = 0.19811446\n",
            "Iteration 286, loss = 0.19751834\n",
            "Iteration 287, loss = 0.19689801\n",
            "Iteration 288, loss = 0.19607505\n",
            "Iteration 289, loss = 0.19541519\n",
            "Iteration 290, loss = 0.19451673\n",
            "Iteration 291, loss = 0.19357777\n",
            "Iteration 292, loss = 0.19284617\n",
            "Iteration 293, loss = 0.19216184\n",
            "Iteration 294, loss = 0.19143295\n",
            "Iteration 295, loss = 0.19064991\n",
            "Iteration 296, loss = 0.18991056\n",
            "Iteration 297, loss = 0.18929598\n",
            "Iteration 298, loss = 0.18875505\n",
            "Iteration 299, loss = 0.18826002\n",
            "Iteration 300, loss = 0.18754971\n",
            "Iteration 301, loss = 0.18668051\n",
            "Iteration 302, loss = 0.18593561\n",
            "Iteration 303, loss = 0.18512448\n",
            "Iteration 304, loss = 0.18448056\n",
            "Iteration 305, loss = 0.18401498\n",
            "Iteration 306, loss = 0.18347511\n",
            "Iteration 307, loss = 0.18296770\n",
            "Iteration 308, loss = 0.18249168\n",
            "Iteration 309, loss = 0.18206344\n",
            "Iteration 310, loss = 0.18149730\n",
            "Iteration 311, loss = 0.18078266\n",
            "Iteration 312, loss = 0.18018719\n",
            "Iteration 313, loss = 0.17932733\n",
            "Iteration 314, loss = 0.17856354\n",
            "Iteration 315, loss = 0.17820089\n",
            "Iteration 316, loss = 0.17762488\n",
            "Iteration 317, loss = 0.17687789\n",
            "Iteration 318, loss = 0.17603877\n",
            "Iteration 319, loss = 0.17510315\n",
            "Iteration 320, loss = 0.17450601\n",
            "Iteration 321, loss = 0.17396689\n",
            "Iteration 322, loss = 0.17333183\n",
            "Iteration 323, loss = 0.17280409\n",
            "Iteration 324, loss = 0.17241946\n",
            "Iteration 325, loss = 0.17202015\n",
            "Iteration 326, loss = 0.17172286\n",
            "Iteration 327, loss = 0.17106178\n",
            "Iteration 328, loss = 0.17034526\n",
            "Iteration 329, loss = 0.16958293\n",
            "Iteration 330, loss = 0.16892674\n",
            "Iteration 331, loss = 0.16822838\n",
            "Iteration 332, loss = 0.16770449\n",
            "Iteration 333, loss = 0.16741441\n",
            "Iteration 334, loss = 0.16730384\n",
            "Iteration 335, loss = 0.16625555\n",
            "Iteration 336, loss = 0.16543342\n",
            "Iteration 337, loss = 0.16486183\n",
            "Iteration 338, loss = 0.16484991\n",
            "Iteration 339, loss = 0.16449703\n",
            "Iteration 340, loss = 0.16390617\n",
            "Iteration 341, loss = 0.16319685\n",
            "Iteration 342, loss = 0.16244662\n",
            "Iteration 343, loss = 0.16177073\n",
            "Iteration 344, loss = 0.16250193\n",
            "Iteration 345, loss = 0.16220037\n",
            "Iteration 346, loss = 0.16099937\n",
            "Iteration 347, loss = 0.15967338\n",
            "Iteration 348, loss = 0.15902919\n",
            "Iteration 349, loss = 0.15935125\n",
            "Iteration 350, loss = 0.15950287\n",
            "Iteration 351, loss = 0.15863406\n",
            "Iteration 352, loss = 0.15727892\n",
            "Iteration 353, loss = 0.15664576\n",
            "Iteration 354, loss = 0.15576038\n",
            "Iteration 355, loss = 0.15551436\n",
            "Iteration 356, loss = 0.15497394\n",
            "Iteration 357, loss = 0.15445017\n",
            "Iteration 358, loss = 0.15357278\n",
            "Iteration 359, loss = 0.15316313\n",
            "Iteration 360, loss = 0.15270586\n",
            "Iteration 361, loss = 0.15234575\n",
            "Iteration 362, loss = 0.15200557\n",
            "Iteration 363, loss = 0.15163866\n",
            "Iteration 364, loss = 0.15127578\n",
            "Iteration 365, loss = 0.15069140\n",
            "Iteration 366, loss = 0.15002968\n",
            "Iteration 367, loss = 0.14943257\n",
            "Iteration 368, loss = 0.14908677\n",
            "Iteration 369, loss = 0.14867205\n",
            "Iteration 370, loss = 0.14811306\n",
            "Iteration 371, loss = 0.14785324\n",
            "Iteration 372, loss = 0.14728690\n",
            "Iteration 373, loss = 0.14690020\n",
            "Iteration 374, loss = 0.14652153\n",
            "Iteration 375, loss = 0.14617706\n",
            "Iteration 376, loss = 0.14571586\n",
            "Iteration 377, loss = 0.14514959\n",
            "Iteration 378, loss = 0.14497702\n",
            "Iteration 379, loss = 0.14483788\n",
            "Iteration 380, loss = 0.14442415\n",
            "Iteration 381, loss = 0.14362190\n",
            "Iteration 382, loss = 0.14304459\n",
            "Iteration 383, loss = 0.14241708\n",
            "Iteration 384, loss = 0.14269311\n",
            "Iteration 385, loss = 0.14278073\n",
            "Iteration 386, loss = 0.14247604\n",
            "Iteration 387, loss = 0.14153128\n",
            "Iteration 388, loss = 0.14043027\n",
            "Iteration 389, loss = 0.13972916\n",
            "Iteration 390, loss = 0.13947105\n",
            "Iteration 391, loss = 0.13904728\n",
            "Iteration 392, loss = 0.13872926\n",
            "Iteration 393, loss = 0.13832576\n",
            "Iteration 394, loss = 0.13792702\n",
            "Iteration 395, loss = 0.13742319\n",
            "Iteration 396, loss = 0.13697609\n",
            "Iteration 397, loss = 0.13632907\n",
            "Iteration 398, loss = 0.13589088\n",
            "Iteration 399, loss = 0.13569909\n",
            "Iteration 400, loss = 0.13569421\n",
            "Iteration 401, loss = 0.13551266\n",
            "Iteration 402, loss = 0.13497208\n",
            "Iteration 403, loss = 0.13443653\n",
            "Iteration 404, loss = 0.13376965\n",
            "Iteration 405, loss = 0.13344804\n",
            "Iteration 406, loss = 0.13329896\n",
            "Iteration 407, loss = 0.13280927\n",
            "Iteration 408, loss = 0.13223471\n",
            "Iteration 409, loss = 0.13182439\n",
            "Iteration 410, loss = 0.13164557\n",
            "Iteration 411, loss = 0.13118405\n",
            "Iteration 412, loss = 0.13075964\n",
            "Iteration 413, loss = 0.13037486\n",
            "Iteration 414, loss = 0.12993583\n",
            "Iteration 415, loss = 0.12935280\n",
            "Iteration 416, loss = 0.12870809\n",
            "Iteration 417, loss = 0.12823353\n",
            "Iteration 418, loss = 0.12832338\n",
            "Iteration 419, loss = 0.12777211\n",
            "Iteration 420, loss = 0.12725069\n",
            "Iteration 421, loss = 0.12686612\n",
            "Iteration 422, loss = 0.12655919\n",
            "Iteration 423, loss = 0.12628948\n",
            "Iteration 424, loss = 0.12603270\n",
            "Iteration 425, loss = 0.12593868\n",
            "Iteration 426, loss = 0.12573682\n",
            "Iteration 427, loss = 0.12537117\n",
            "Iteration 428, loss = 0.12499160\n",
            "Iteration 429, loss = 0.12452803\n",
            "Iteration 430, loss = 0.12379690\n",
            "Iteration 431, loss = 0.12365661\n",
            "Iteration 432, loss = 0.12380170\n",
            "Iteration 433, loss = 0.12349731\n",
            "Iteration 434, loss = 0.12309994\n",
            "Iteration 435, loss = 0.12260741\n",
            "Iteration 436, loss = 0.12211874\n",
            "Iteration 437, loss = 0.12164742\n",
            "Iteration 438, loss = 0.12135512\n",
            "Iteration 439, loss = 0.12103616\n",
            "Iteration 440, loss = 0.12070662\n",
            "Iteration 441, loss = 0.12043941\n",
            "Iteration 442, loss = 0.11995565\n",
            "Iteration 443, loss = 0.11959294\n",
            "Iteration 444, loss = 0.11907689\n",
            "Iteration 445, loss = 0.11869015\n",
            "Iteration 446, loss = 0.11843171\n",
            "Iteration 447, loss = 0.11827647\n",
            "Iteration 448, loss = 0.11836311\n",
            "Iteration 449, loss = 0.11831756\n",
            "Iteration 450, loss = 0.11750496\n",
            "Iteration 451, loss = 0.11663424\n",
            "Iteration 452, loss = 0.11648656\n",
            "Iteration 453, loss = 0.11633469\n",
            "Iteration 454, loss = 0.11617314\n",
            "Iteration 455, loss = 0.11567531\n",
            "Iteration 456, loss = 0.11496983\n",
            "Iteration 457, loss = 0.11450089\n",
            "Iteration 458, loss = 0.11447283\n",
            "Iteration 459, loss = 0.11409431\n",
            "Iteration 460, loss = 0.11374317\n",
            "Iteration 461, loss = 0.11317015\n",
            "Iteration 462, loss = 0.11288927\n",
            "Iteration 463, loss = 0.11261251\n",
            "Iteration 464, loss = 0.11244196\n",
            "Iteration 465, loss = 0.11220539\n",
            "Iteration 466, loss = 0.11205831\n",
            "Iteration 467, loss = 0.11181896\n",
            "Iteration 468, loss = 0.11156880\n",
            "Iteration 469, loss = 0.11131691\n",
            "Iteration 470, loss = 0.11099281\n",
            "Iteration 471, loss = 0.11080683\n",
            "Iteration 472, loss = 0.11055529\n",
            "Iteration 473, loss = 0.11017952\n",
            "Iteration 474, loss = 0.10982055\n",
            "Iteration 475, loss = 0.10936611\n",
            "Iteration 476, loss = 0.10917635\n",
            "Iteration 477, loss = 0.10878656\n",
            "Iteration 478, loss = 0.10864706\n",
            "Iteration 479, loss = 0.10832441\n",
            "Iteration 480, loss = 0.10796642\n",
            "Iteration 481, loss = 0.10764648\n",
            "Iteration 482, loss = 0.10745969\n",
            "Iteration 483, loss = 0.10709211\n",
            "Iteration 484, loss = 0.10676537\n",
            "Iteration 485, loss = 0.10650840\n",
            "Iteration 486, loss = 0.10604898\n",
            "Iteration 487, loss = 0.10561993\n",
            "Iteration 488, loss = 0.10515372\n",
            "Iteration 489, loss = 0.10438060\n",
            "Iteration 490, loss = 0.10423186\n",
            "Iteration 491, loss = 0.10467888\n",
            "Iteration 492, loss = 0.10557959\n",
            "Iteration 493, loss = 0.10506383\n",
            "Iteration 494, loss = 0.10384358\n",
            "Iteration 495, loss = 0.10301173\n",
            "Iteration 496, loss = 0.10242589\n",
            "Iteration 497, loss = 0.10247779\n",
            "Iteration 498, loss = 0.10242395\n",
            "Iteration 499, loss = 0.10223896\n",
            "Iteration 500, loss = 0.10194849\n",
            "Iteration 501, loss = 0.10167047\n",
            "Iteration 502, loss = 0.10148142\n",
            "Iteration 503, loss = 0.10099133\n",
            "Iteration 504, loss = 0.10032920\n",
            "Iteration 505, loss = 0.09960574\n",
            "Iteration 506, loss = 0.09931378\n",
            "Iteration 507, loss = 0.09921598\n",
            "Iteration 508, loss = 0.09917923\n",
            "Iteration 509, loss = 0.09909431\n",
            "Iteration 510, loss = 0.09876955\n",
            "Iteration 511, loss = 0.09826811\n",
            "Iteration 512, loss = 0.09799845\n",
            "Iteration 513, loss = 0.09758334\n",
            "Iteration 514, loss = 0.09727031\n",
            "Iteration 515, loss = 0.09695490\n",
            "Iteration 516, loss = 0.09695510\n",
            "Iteration 517, loss = 0.09723114\n",
            "Iteration 518, loss = 0.09700089\n",
            "Iteration 519, loss = 0.09678499\n",
            "Iteration 520, loss = 0.09639831\n",
            "Iteration 521, loss = 0.09601190\n",
            "Iteration 522, loss = 0.09581142\n",
            "Iteration 523, loss = 0.09545854\n",
            "Iteration 524, loss = 0.09526685\n",
            "Iteration 525, loss = 0.09512076\n",
            "Iteration 526, loss = 0.09494941\n",
            "Iteration 527, loss = 0.09476062\n",
            "Iteration 528, loss = 0.09456184\n",
            "Iteration 529, loss = 0.09431400\n",
            "Iteration 530, loss = 0.09406400\n",
            "Iteration 531, loss = 0.09446089\n",
            "Iteration 532, loss = 0.09440711\n",
            "Iteration 533, loss = 0.09402242\n",
            "Iteration 534, loss = 0.09341819\n",
            "Iteration 535, loss = 0.09306484\n",
            "Iteration 536, loss = 0.09271265\n",
            "Iteration 537, loss = 0.09251144\n",
            "Iteration 538, loss = 0.09226833\n",
            "Iteration 539, loss = 0.09200153\n",
            "Iteration 540, loss = 0.09169607\n",
            "Iteration 541, loss = 0.09152643\n",
            "Iteration 542, loss = 0.09127783\n",
            "Iteration 543, loss = 0.09104690\n",
            "Iteration 544, loss = 0.09095247\n",
            "Iteration 545, loss = 0.09072949\n",
            "Iteration 546, loss = 0.09045434\n",
            "Iteration 547, loss = 0.09044308\n",
            "Iteration 548, loss = 0.09056990\n",
            "Iteration 549, loss = 0.09052608\n",
            "Iteration 550, loss = 0.09033886\n",
            "Iteration 551, loss = 0.08988700\n",
            "Iteration 552, loss = 0.08943493\n",
            "Iteration 553, loss = 0.08917300\n",
            "Iteration 554, loss = 0.08888575\n",
            "Iteration 555, loss = 0.08857660\n",
            "Iteration 556, loss = 0.08831927\n",
            "Iteration 557, loss = 0.08794008\n",
            "Iteration 558, loss = 0.08754754\n",
            "Iteration 559, loss = 0.08722677\n",
            "Iteration 560, loss = 0.08698318\n",
            "Iteration 561, loss = 0.08675095\n",
            "Iteration 562, loss = 0.08643023\n",
            "Iteration 563, loss = 0.08620680\n",
            "Iteration 564, loss = 0.08581514\n",
            "Iteration 565, loss = 0.08553985\n",
            "Iteration 566, loss = 0.08534332\n",
            "Iteration 567, loss = 0.08503947\n",
            "Iteration 568, loss = 0.08482841\n",
            "Iteration 569, loss = 0.08477253\n",
            "Iteration 570, loss = 0.08442148\n",
            "Iteration 571, loss = 0.08421861\n",
            "Iteration 572, loss = 0.08404310\n",
            "Iteration 573, loss = 0.08390492\n",
            "Iteration 574, loss = 0.08370198\n",
            "Iteration 575, loss = 0.08359538\n",
            "Iteration 576, loss = 0.08345764\n",
            "Iteration 577, loss = 0.08315839\n",
            "Iteration 578, loss = 0.08312282\n",
            "Iteration 579, loss = 0.08295278\n",
            "Iteration 580, loss = 0.08288903\n",
            "Iteration 581, loss = 0.08263005\n",
            "Iteration 582, loss = 0.08242728\n",
            "Iteration 583, loss = 0.08234840\n",
            "Iteration 584, loss = 0.08192091\n",
            "Iteration 585, loss = 0.08166045\n",
            "Iteration 586, loss = 0.08128627\n",
            "Iteration 587, loss = 0.08099438\n",
            "Iteration 588, loss = 0.08088598\n",
            "Iteration 589, loss = 0.08072314\n",
            "Iteration 590, loss = 0.08068380\n",
            "Iteration 591, loss = 0.08055446\n",
            "Iteration 592, loss = 0.08045782\n",
            "Iteration 593, loss = 0.08030625\n",
            "Iteration 594, loss = 0.08012997\n",
            "Iteration 595, loss = 0.07990657\n",
            "Iteration 596, loss = 0.07953668\n",
            "Iteration 597, loss = 0.07923936\n",
            "Iteration 598, loss = 0.07938227\n",
            "Iteration 599, loss = 0.07944736\n",
            "Iteration 600, loss = 0.07936966\n",
            "Iteration 601, loss = 0.07924163\n",
            "Iteration 602, loss = 0.07897444\n",
            "Iteration 603, loss = 0.07860871\n",
            "Iteration 604, loss = 0.07828293\n",
            "Iteration 605, loss = 0.07819638\n",
            "Iteration 606, loss = 0.07860530\n",
            "Iteration 607, loss = 0.07842698\n",
            "Iteration 608, loss = 0.07803598\n",
            "Iteration 609, loss = 0.07767194\n",
            "Iteration 610, loss = 0.07729421\n",
            "Iteration 611, loss = 0.07718835\n",
            "Iteration 612, loss = 0.07748916\n",
            "Iteration 613, loss = 0.07757403\n",
            "Iteration 614, loss = 0.07751093\n",
            "Iteration 615, loss = 0.07748623\n",
            "Iteration 616, loss = 0.07718430\n",
            "Iteration 617, loss = 0.07675890\n",
            "Iteration 618, loss = 0.07634719\n",
            "Iteration 619, loss = 0.07605242\n",
            "Iteration 620, loss = 0.07586891\n",
            "Iteration 621, loss = 0.07572204\n",
            "Iteration 622, loss = 0.07563836\n",
            "Iteration 623, loss = 0.07563340\n",
            "Iteration 624, loss = 0.07538676\n",
            "Iteration 625, loss = 0.07533880\n",
            "Iteration 626, loss = 0.07568122\n",
            "Iteration 627, loss = 0.07572521\n",
            "Iteration 628, loss = 0.07549983\n",
            "Iteration 629, loss = 0.07508516\n",
            "Iteration 630, loss = 0.07464270\n",
            "Iteration 631, loss = 0.07432457\n",
            "Iteration 632, loss = 0.07386104\n",
            "Iteration 633, loss = 0.07367962\n",
            "Iteration 634, loss = 0.07370893\n",
            "Iteration 635, loss = 0.07386296\n",
            "Iteration 636, loss = 0.07424859\n",
            "Iteration 637, loss = 0.07425928\n",
            "Iteration 638, loss = 0.07388299\n",
            "Iteration 639, loss = 0.07319454\n",
            "Iteration 640, loss = 0.07276199\n",
            "Iteration 641, loss = 0.07244711\n",
            "Iteration 642, loss = 0.07206624\n",
            "Iteration 643, loss = 0.07220759\n",
            "Iteration 644, loss = 0.07316690\n",
            "Iteration 645, loss = 0.07341968\n",
            "Iteration 646, loss = 0.07317858\n",
            "Iteration 647, loss = 0.07262086\n",
            "Iteration 648, loss = 0.07211154\n",
            "Iteration 649, loss = 0.07192317\n",
            "Iteration 650, loss = 0.07147597\n",
            "Iteration 651, loss = 0.07118131\n",
            "Iteration 652, loss = 0.07134206\n",
            "Iteration 653, loss = 0.07130663\n",
            "Iteration 654, loss = 0.07121539\n",
            "Iteration 655, loss = 0.07098460\n",
            "Iteration 656, loss = 0.07075733\n",
            "Iteration 657, loss = 0.07047507\n",
            "Iteration 658, loss = 0.07019037\n",
            "Iteration 659, loss = 0.07013079\n",
            "Iteration 660, loss = 0.07033899\n",
            "Iteration 661, loss = 0.07043163\n",
            "Iteration 662, loss = 0.07036312\n",
            "Iteration 663, loss = 0.07038907\n",
            "Iteration 664, loss = 0.07048990\n",
            "Iteration 665, loss = 0.07010363\n",
            "Iteration 666, loss = 0.06967943\n",
            "Iteration 667, loss = 0.06927726\n",
            "Iteration 668, loss = 0.06891410\n",
            "Iteration 669, loss = 0.06857874\n",
            "Iteration 670, loss = 0.06859011\n",
            "Iteration 671, loss = 0.06843388\n",
            "Iteration 672, loss = 0.06862000\n",
            "Iteration 673, loss = 0.06908024\n",
            "Iteration 674, loss = 0.06929281\n",
            "Iteration 675, loss = 0.06922195\n",
            "Iteration 676, loss = 0.06914594\n",
            "Iteration 677, loss = 0.06845813\n",
            "Iteration 678, loss = 0.06784168\n",
            "Iteration 679, loss = 0.06784303\n",
            "Iteration 680, loss = 0.06775669\n",
            "Iteration 681, loss = 0.06756367\n",
            "Iteration 682, loss = 0.06732521\n",
            "Iteration 683, loss = 0.06715084\n",
            "Iteration 684, loss = 0.06719932\n",
            "Iteration 685, loss = 0.06752299\n",
            "Iteration 686, loss = 0.06740411\n",
            "Iteration 687, loss = 0.06714772\n",
            "Iteration 688, loss = 0.06687380\n",
            "Iteration 689, loss = 0.06661802\n",
            "Iteration 690, loss = 0.06640112\n",
            "Iteration 691, loss = 0.06615629\n",
            "Iteration 692, loss = 0.06602497\n",
            "Iteration 693, loss = 0.06575910\n",
            "Iteration 694, loss = 0.06546320\n",
            "Iteration 695, loss = 0.06521883\n",
            "Iteration 696, loss = 0.06513702\n",
            "Iteration 697, loss = 0.06493147\n",
            "Iteration 698, loss = 0.06479348\n",
            "Iteration 699, loss = 0.06459310\n",
            "Iteration 700, loss = 0.06456365\n",
            "Iteration 701, loss = 0.06444332\n",
            "Iteration 702, loss = 0.06433476\n",
            "Iteration 703, loss = 0.06420259\n",
            "Iteration 704, loss = 0.06403779\n",
            "Iteration 705, loss = 0.06385706\n",
            "Iteration 706, loss = 0.06369643\n",
            "Iteration 707, loss = 0.06365560\n",
            "Iteration 708, loss = 0.06345162\n",
            "Iteration 709, loss = 0.06316328\n",
            "Iteration 710, loss = 0.06296842\n",
            "Iteration 711, loss = 0.06290213\n",
            "Iteration 712, loss = 0.06282244\n",
            "Iteration 713, loss = 0.06274324\n",
            "Iteration 714, loss = 0.06273340\n",
            "Iteration 715, loss = 0.06270782\n",
            "Iteration 716, loss = 0.06265226\n",
            "Iteration 717, loss = 0.06260781\n",
            "Iteration 718, loss = 0.06234379\n",
            "Iteration 719, loss = 0.06218013\n",
            "Iteration 720, loss = 0.06206707\n",
            "Iteration 721, loss = 0.06189486\n",
            "Iteration 722, loss = 0.06175385\n",
            "Iteration 723, loss = 0.06166725\n",
            "Iteration 724, loss = 0.06150892\n",
            "Iteration 725, loss = 0.06135096\n",
            "Iteration 726, loss = 0.06121072\n",
            "Iteration 727, loss = 0.06112707\n",
            "Iteration 728, loss = 0.06101283\n",
            "Iteration 729, loss = 0.06083778\n",
            "Iteration 730, loss = 0.06079887\n",
            "Iteration 731, loss = 0.06080247\n",
            "Iteration 732, loss = 0.06086983\n",
            "Iteration 733, loss = 0.06067476\n",
            "Iteration 734, loss = 0.06038383\n",
            "Iteration 735, loss = 0.06024671\n",
            "Iteration 736, loss = 0.06040273\n",
            "Iteration 737, loss = 0.06045516\n",
            "Iteration 738, loss = 0.06050425\n",
            "Iteration 739, loss = 0.06066386\n",
            "Iteration 740, loss = 0.06063539\n",
            "Iteration 741, loss = 0.06027729\n",
            "Iteration 742, loss = 0.05973052\n",
            "Iteration 743, loss = 0.05953069\n",
            "Iteration 744, loss = 0.05947436\n",
            "Iteration 745, loss = 0.05944064\n",
            "Iteration 746, loss = 0.05942835\n",
            "Iteration 747, loss = 0.05957505\n",
            "Iteration 748, loss = 0.05937444\n",
            "Iteration 749, loss = 0.05910641\n",
            "Iteration 750, loss = 0.05883590\n",
            "Iteration 751, loss = 0.05870121\n",
            "Iteration 752, loss = 0.05868696\n",
            "Iteration 753, loss = 0.05914221\n",
            "Iteration 754, loss = 0.05962244\n",
            "Iteration 755, loss = 0.05968890\n",
            "Iteration 756, loss = 0.05912590\n",
            "Iteration 757, loss = 0.05850735\n",
            "Iteration 758, loss = 0.05818181\n",
            "Iteration 759, loss = 0.05800654\n",
            "Iteration 760, loss = 0.05789061\n",
            "Iteration 761, loss = 0.05783759\n",
            "Iteration 762, loss = 0.05772457\n",
            "Iteration 763, loss = 0.05761512\n",
            "Iteration 764, loss = 0.05752388\n",
            "Iteration 765, loss = 0.05748123\n",
            "Iteration 766, loss = 0.05729056\n",
            "Iteration 767, loss = 0.05714037\n",
            "Iteration 768, loss = 0.05702553\n",
            "Iteration 769, loss = 0.05684705\n",
            "Iteration 770, loss = 0.05686850\n",
            "Iteration 771, loss = 0.05681080\n",
            "Iteration 772, loss = 0.05670788\n",
            "Iteration 773, loss = 0.05663932\n",
            "Iteration 774, loss = 0.05655610\n",
            "Iteration 775, loss = 0.05649025\n",
            "Iteration 776, loss = 0.05643414\n",
            "Iteration 777, loss = 0.05635294\n",
            "Iteration 778, loss = 0.05633003\n",
            "Iteration 779, loss = 0.05633929\n",
            "Iteration 780, loss = 0.05623857\n",
            "Iteration 781, loss = 0.05592996\n",
            "Iteration 782, loss = 0.05586906\n",
            "Iteration 783, loss = 0.05572606\n",
            "Iteration 784, loss = 0.05562101\n",
            "Iteration 785, loss = 0.05550792\n",
            "Iteration 786, loss = 0.05567816\n",
            "Iteration 787, loss = 0.05551520\n",
            "Iteration 788, loss = 0.05551084\n",
            "Iteration 789, loss = 0.05549447\n",
            "Iteration 790, loss = 0.05561588\n",
            "Iteration 791, loss = 0.05574600\n",
            "Iteration 792, loss = 0.05587251\n",
            "Iteration 793, loss = 0.05584817\n",
            "Iteration 794, loss = 0.05570360\n",
            "Iteration 795, loss = 0.05551377\n",
            "Iteration 796, loss = 0.05518514\n",
            "Iteration 797, loss = 0.05492356\n",
            "Iteration 798, loss = 0.05465856\n",
            "Iteration 799, loss = 0.05456231\n",
            "Iteration 800, loss = 0.05460907\n",
            "Iteration 801, loss = 0.05471039\n",
            "Iteration 802, loss = 0.05461380\n",
            "Iteration 803, loss = 0.05445833\n",
            "Iteration 804, loss = 0.05443341\n",
            "Iteration 805, loss = 0.05428780\n",
            "Iteration 806, loss = 0.05427432\n",
            "Iteration 807, loss = 0.05405058\n",
            "Iteration 808, loss = 0.05384712\n",
            "Iteration 809, loss = 0.05367847\n",
            "Iteration 810, loss = 0.05358398\n",
            "Iteration 811, loss = 0.05347084\n",
            "Iteration 812, loss = 0.05355412\n",
            "Iteration 813, loss = 0.05354642\n",
            "Iteration 814, loss = 0.05379276\n",
            "Iteration 815, loss = 0.05400052\n",
            "Iteration 816, loss = 0.05379189\n",
            "Iteration 817, loss = 0.05337321\n",
            "Iteration 818, loss = 0.05327152\n",
            "Iteration 819, loss = 0.05285515\n",
            "Iteration 820, loss = 0.05266727\n",
            "Iteration 821, loss = 0.05254867\n",
            "Iteration 822, loss = 0.05239772\n",
            "Iteration 823, loss = 0.05235634\n",
            "Iteration 824, loss = 0.05234947\n",
            "Iteration 825, loss = 0.05260810\n",
            "Iteration 826, loss = 0.05252531\n",
            "Iteration 827, loss = 0.05242204\n",
            "Iteration 828, loss = 0.05239894\n",
            "Iteration 829, loss = 0.05238753\n",
            "Iteration 830, loss = 0.05262689\n",
            "Iteration 831, loss = 0.05229629\n",
            "Iteration 832, loss = 0.05209195\n",
            "Iteration 833, loss = 0.05210415\n",
            "Iteration 834, loss = 0.05227522\n",
            "Iteration 835, loss = 0.05206476\n",
            "Iteration 836, loss = 0.05181329\n",
            "Iteration 837, loss = 0.05178830\n",
            "Iteration 838, loss = 0.05154752\n",
            "Iteration 839, loss = 0.05144679\n",
            "Iteration 840, loss = 0.05132264\n",
            "Iteration 841, loss = 0.05144860\n",
            "Iteration 842, loss = 0.05121572\n",
            "Iteration 843, loss = 0.05103368\n",
            "Iteration 844, loss = 0.05109747\n",
            "Iteration 845, loss = 0.05140017\n",
            "Iteration 846, loss = 0.05141727\n",
            "Iteration 847, loss = 0.05114792\n",
            "Iteration 848, loss = 0.05095768\n",
            "Iteration 849, loss = 0.05055282\n",
            "Iteration 850, loss = 0.05051086\n",
            "Iteration 851, loss = 0.05060932\n",
            "Iteration 852, loss = 0.05037048\n",
            "Iteration 853, loss = 0.05029093\n",
            "Iteration 854, loss = 0.05027227\n",
            "Iteration 855, loss = 0.05012807\n",
            "Iteration 856, loss = 0.05000915\n",
            "Iteration 857, loss = 0.04998202\n",
            "Iteration 858, loss = 0.04981984\n",
            "Iteration 859, loss = 0.04968881\n",
            "Iteration 860, loss = 0.04966013\n",
            "Iteration 861, loss = 0.04960904\n",
            "Iteration 862, loss = 0.04938767\n",
            "Iteration 863, loss = 0.04937489\n",
            "Iteration 864, loss = 0.04948377\n",
            "Iteration 865, loss = 0.04991894\n",
            "Iteration 866, loss = 0.05003203\n",
            "Iteration 867, loss = 0.05003079\n",
            "Iteration 868, loss = 0.04995493\n",
            "Iteration 869, loss = 0.04983707\n",
            "Iteration 870, loss = 0.04973869\n",
            "Iteration 871, loss = 0.04945417\n",
            "Iteration 872, loss = 0.04940746\n",
            "Iteration 873, loss = 0.04931023\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelo = MLPClassifier(max_iter=1000, verbose=True)\n",
        "modelo.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PQZVMhEJXOC"
      },
      "source": [
        "**Faça outras alterações nos parâmetros**\n",
        "\n",
        "**4 entradas - 3 neurônios - 3 neurônios - 1**\n",
        "\n",
        "**Veja SoftMax para problemas multiclasse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Q1RssrJU9z",
        "outputId": "802b6904-3a64-4a71-fd3b-221c672f3524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.72306035\n",
            "Iteration 2, loss = 0.69633308\n",
            "Iteration 3, loss = 0.67320514\n",
            "Iteration 4, loss = 0.65343187\n",
            "Iteration 5, loss = 0.63608752\n",
            "Iteration 6, loss = 0.62157027\n",
            "Iteration 7, loss = 0.60919457\n",
            "Iteration 8, loss = 0.59918153\n",
            "Iteration 9, loss = 0.58984938\n",
            "Iteration 10, loss = 0.58222365\n",
            "Iteration 11, loss = 0.57624203\n",
            "Iteration 12, loss = 0.57043246\n",
            "Iteration 13, loss = 0.56483057\n",
            "Iteration 14, loss = 0.56047715\n",
            "Iteration 15, loss = 0.55570116\n",
            "Iteration 16, loss = 0.55150399\n",
            "Iteration 17, loss = 0.54718731\n",
            "Iteration 18, loss = 0.54324718\n",
            "Iteration 19, loss = 0.53924800\n",
            "Iteration 20, loss = 0.53531778\n",
            "Iteration 21, loss = 0.53172883\n",
            "Iteration 22, loss = 0.52814964\n",
            "Iteration 23, loss = 0.52485392\n",
            "Iteration 24, loss = 0.52152478\n",
            "Iteration 25, loss = 0.51828858\n",
            "Iteration 26, loss = 0.51524400\n",
            "Iteration 27, loss = 0.51197622\n",
            "Iteration 28, loss = 0.50893082\n",
            "Iteration 29, loss = 0.50604187\n",
            "Iteration 30, loss = 0.50335033\n",
            "Iteration 31, loss = 0.50056205\n",
            "Iteration 32, loss = 0.49829514\n",
            "Iteration 33, loss = 0.49588888\n",
            "Iteration 34, loss = 0.49348036\n",
            "Iteration 35, loss = 0.49101827\n",
            "Iteration 36, loss = 0.48857963\n",
            "Iteration 37, loss = 0.48624603\n",
            "Iteration 38, loss = 0.48376188\n",
            "Iteration 39, loss = 0.48106891\n",
            "Iteration 40, loss = 0.47847430\n",
            "Iteration 41, loss = 0.47586977\n",
            "Iteration 42, loss = 0.47349219\n",
            "Iteration 43, loss = 0.47154989\n",
            "Iteration 44, loss = 0.46937815\n",
            "Iteration 45, loss = 0.46732053\n",
            "Iteration 46, loss = 0.46534331\n",
            "Iteration 47, loss = 0.46350366\n",
            "Iteration 48, loss = 0.46144615\n",
            "Iteration 49, loss = 0.45944706\n",
            "Iteration 50, loss = 0.45792240\n",
            "Iteration 51, loss = 0.45603998\n",
            "Iteration 52, loss = 0.45434814\n",
            "Iteration 53, loss = 0.45265301\n",
            "Iteration 54, loss = 0.45090082\n",
            "Iteration 55, loss = 0.44922907\n",
            "Iteration 56, loss = 0.44777613\n",
            "Iteration 57, loss = 0.44606487\n",
            "Iteration 58, loss = 0.44452422\n",
            "Iteration 59, loss = 0.44307456\n",
            "Iteration 60, loss = 0.44164743\n",
            "Iteration 61, loss = 0.44022808\n",
            "Iteration 62, loss = 0.43864593\n",
            "Iteration 63, loss = 0.43698198\n",
            "Iteration 64, loss = 0.43549209\n",
            "Iteration 65, loss = 0.43400155\n",
            "Iteration 66, loss = 0.43240155\n",
            "Iteration 67, loss = 0.43087825\n",
            "Iteration 68, loss = 0.42928122\n",
            "Iteration 69, loss = 0.42776678\n",
            "Iteration 70, loss = 0.42635682\n",
            "Iteration 71, loss = 0.42505359\n",
            "Iteration 72, loss = 0.42349959\n",
            "Iteration 73, loss = 0.42202989\n",
            "Iteration 74, loss = 0.42071459\n",
            "Iteration 75, loss = 0.41922027\n",
            "Iteration 76, loss = 0.41775358\n",
            "Iteration 77, loss = 0.41652875\n",
            "Iteration 78, loss = 0.41512882\n",
            "Iteration 79, loss = 0.41386907\n",
            "Iteration 80, loss = 0.41224781\n",
            "Iteration 81, loss = 0.41082969\n",
            "Iteration 82, loss = 0.40935469\n",
            "Iteration 83, loss = 0.40775415\n",
            "Iteration 84, loss = 0.40611159\n",
            "Iteration 85, loss = 0.40465835\n",
            "Iteration 86, loss = 0.40353939\n",
            "Iteration 87, loss = 0.40231442\n",
            "Iteration 88, loss = 0.40086839\n",
            "Iteration 89, loss = 0.39930299\n",
            "Iteration 90, loss = 0.39778984\n",
            "Iteration 91, loss = 0.39633180\n",
            "Iteration 92, loss = 0.39473205\n",
            "Iteration 93, loss = 0.39332329\n",
            "Iteration 94, loss = 0.39197797\n",
            "Iteration 95, loss = 0.39060623\n",
            "Iteration 96, loss = 0.38914970\n",
            "Iteration 97, loss = 0.38769257\n",
            "Iteration 98, loss = 0.38622746\n",
            "Iteration 99, loss = 0.38464386\n",
            "Iteration 100, loss = 0.38351029\n",
            "Iteration 101, loss = 0.38223333\n",
            "Iteration 102, loss = 0.38117908\n",
            "Iteration 103, loss = 0.38017264\n",
            "Iteration 104, loss = 0.37838484\n",
            "Iteration 105, loss = 0.37661316\n",
            "Iteration 106, loss = 0.37501066\n",
            "Iteration 107, loss = 0.37390568\n",
            "Iteration 108, loss = 0.37231343\n",
            "Iteration 109, loss = 0.37084845\n",
            "Iteration 110, loss = 0.36930879\n",
            "Iteration 111, loss = 0.36774441\n",
            "Iteration 112, loss = 0.36637167\n",
            "Iteration 113, loss = 0.36524841\n",
            "Iteration 114, loss = 0.36416797\n",
            "Iteration 115, loss = 0.36352525\n",
            "Iteration 116, loss = 0.36240069\n",
            "Iteration 117, loss = 0.36078396\n",
            "Iteration 118, loss = 0.35917152\n",
            "Iteration 119, loss = 0.35753453\n",
            "Iteration 120, loss = 0.35600500\n",
            "Iteration 121, loss = 0.35428524\n",
            "Iteration 122, loss = 0.35268164\n",
            "Iteration 123, loss = 0.35109078\n",
            "Iteration 124, loss = 0.34960645\n",
            "Iteration 125, loss = 0.34834941\n",
            "Iteration 126, loss = 0.34686832\n",
            "Iteration 127, loss = 0.34552436\n",
            "Iteration 128, loss = 0.34428952\n",
            "Iteration 129, loss = 0.34304752\n",
            "Iteration 130, loss = 0.34169101\n",
            "Iteration 131, loss = 0.34029158\n",
            "Iteration 132, loss = 0.33900742\n",
            "Iteration 133, loss = 0.33781937\n",
            "Iteration 134, loss = 0.33662568\n",
            "Iteration 135, loss = 0.33526648\n",
            "Iteration 136, loss = 0.33416667\n",
            "Iteration 137, loss = 0.33295943\n",
            "Iteration 138, loss = 0.33169658\n",
            "Iteration 139, loss = 0.33026739\n",
            "Iteration 140, loss = 0.32893757\n",
            "Iteration 141, loss = 0.32769266\n",
            "Iteration 142, loss = 0.32623374\n",
            "Iteration 143, loss = 0.32492536\n",
            "Iteration 144, loss = 0.32354807\n",
            "Iteration 145, loss = 0.32225383\n",
            "Iteration 146, loss = 0.32092440\n",
            "Iteration 147, loss = 0.31953908\n",
            "Iteration 148, loss = 0.31829409\n",
            "Iteration 149, loss = 0.31692842\n",
            "Iteration 150, loss = 0.31639302\n",
            "Iteration 151, loss = 0.31478920\n",
            "Iteration 152, loss = 0.31317505\n",
            "Iteration 153, loss = 0.31142711\n",
            "Iteration 154, loss = 0.30990118\n",
            "Iteration 155, loss = 0.30886387\n",
            "Iteration 156, loss = 0.30783565\n",
            "Iteration 157, loss = 0.30684641\n",
            "Iteration 158, loss = 0.30572590\n",
            "Iteration 159, loss = 0.30428905\n",
            "Iteration 160, loss = 0.30278351\n",
            "Iteration 161, loss = 0.30110964\n",
            "Iteration 162, loss = 0.29950707\n",
            "Iteration 163, loss = 0.29818382\n",
            "Iteration 164, loss = 0.29697570\n",
            "Iteration 165, loss = 0.29575100\n",
            "Iteration 166, loss = 0.29445169\n",
            "Iteration 167, loss = 0.29321168\n",
            "Iteration 168, loss = 0.29201269\n",
            "Iteration 169, loss = 0.29050594\n",
            "Iteration 170, loss = 0.28919206\n",
            "Iteration 171, loss = 0.28786486\n",
            "Iteration 172, loss = 0.28688682\n",
            "Iteration 173, loss = 0.28574270\n",
            "Iteration 174, loss = 0.28446725\n",
            "Iteration 175, loss = 0.28321651\n",
            "Iteration 176, loss = 0.28216078\n",
            "Iteration 177, loss = 0.28095062\n",
            "Iteration 178, loss = 0.27939966\n",
            "Iteration 179, loss = 0.27832308\n",
            "Iteration 180, loss = 0.27702349\n",
            "Iteration 181, loss = 0.27605231\n",
            "Iteration 182, loss = 0.27534741\n",
            "Iteration 183, loss = 0.27439521\n",
            "Iteration 184, loss = 0.27358255\n",
            "Iteration 185, loss = 0.27259614\n",
            "Iteration 186, loss = 0.27181551\n",
            "Iteration 187, loss = 0.27085102\n",
            "Iteration 188, loss = 0.26950385\n",
            "Iteration 189, loss = 0.26814267\n",
            "Iteration 190, loss = 0.26648412\n",
            "Iteration 191, loss = 0.26481779\n",
            "Iteration 192, loss = 0.26477722\n",
            "Iteration 193, loss = 0.26423883\n",
            "Iteration 194, loss = 0.26335843\n",
            "Iteration 195, loss = 0.26197912\n",
            "Iteration 196, loss = 0.26063552\n",
            "Iteration 197, loss = 0.25908899\n",
            "Iteration 198, loss = 0.25782271\n",
            "Iteration 199, loss = 0.25667254\n",
            "Iteration 200, loss = 0.25549232\n",
            "Iteration 201, loss = 0.25438703\n",
            "Iteration 202, loss = 0.25333116\n",
            "Iteration 203, loss = 0.25221451\n",
            "Iteration 204, loss = 0.25119086\n",
            "Iteration 205, loss = 0.25005236\n",
            "Iteration 206, loss = 0.24878739\n",
            "Iteration 207, loss = 0.24755890\n",
            "Iteration 208, loss = 0.24640053\n",
            "Iteration 209, loss = 0.24541269\n",
            "Iteration 210, loss = 0.24440067\n",
            "Iteration 211, loss = 0.24338512\n",
            "Iteration 212, loss = 0.24227527\n",
            "Iteration 213, loss = 0.24110671\n",
            "Iteration 214, loss = 0.24044229\n",
            "Iteration 215, loss = 0.24011458\n",
            "Iteration 216, loss = 0.23959490\n",
            "Iteration 217, loss = 0.23892040\n",
            "Iteration 218, loss = 0.23823594\n",
            "Iteration 219, loss = 0.23724096\n",
            "Iteration 220, loss = 0.23604891\n",
            "Iteration 221, loss = 0.23504411\n",
            "Iteration 222, loss = 0.23416202\n",
            "Iteration 223, loss = 0.23316072\n",
            "Iteration 224, loss = 0.23239321\n",
            "Iteration 225, loss = 0.23142583\n",
            "Iteration 226, loss = 0.23020865\n",
            "Iteration 227, loss = 0.22940873\n",
            "Iteration 228, loss = 0.22897231\n",
            "Iteration 229, loss = 0.22953148\n",
            "Iteration 230, loss = 0.22864851\n",
            "Iteration 231, loss = 0.22684197\n",
            "Iteration 232, loss = 0.22458960\n",
            "Iteration 233, loss = 0.22307218\n",
            "Iteration 234, loss = 0.22270068\n",
            "Iteration 235, loss = 0.22266335\n",
            "Iteration 236, loss = 0.22238120\n",
            "Iteration 237, loss = 0.22181444\n",
            "Iteration 238, loss = 0.21967359\n",
            "Iteration 239, loss = 0.21824850\n",
            "Iteration 240, loss = 0.21772723\n",
            "Iteration 241, loss = 0.21692938\n",
            "Iteration 242, loss = 0.21585922\n",
            "Iteration 243, loss = 0.21464406\n",
            "Iteration 244, loss = 0.21361679\n",
            "Iteration 245, loss = 0.21273443\n",
            "Iteration 246, loss = 0.21181645\n",
            "Iteration 247, loss = 0.21084135\n",
            "Iteration 248, loss = 0.20995920\n",
            "Iteration 249, loss = 0.20926255\n",
            "Iteration 250, loss = 0.20876684\n",
            "Iteration 251, loss = 0.20807065\n",
            "Iteration 252, loss = 0.20720668\n",
            "Iteration 253, loss = 0.20620037\n",
            "Iteration 254, loss = 0.20498611\n",
            "Iteration 255, loss = 0.20393995\n",
            "Iteration 256, loss = 0.20296673\n",
            "Iteration 257, loss = 0.20228668\n",
            "Iteration 258, loss = 0.20158296\n",
            "Iteration 259, loss = 0.20114813\n",
            "Iteration 260, loss = 0.20048518\n",
            "Iteration 261, loss = 0.19930850\n",
            "Iteration 262, loss = 0.19823179\n",
            "Iteration 263, loss = 0.19740320\n",
            "Iteration 264, loss = 0.19674026\n",
            "Iteration 265, loss = 0.19633543\n",
            "Iteration 266, loss = 0.19545535\n",
            "Iteration 267, loss = 0.19426558\n",
            "Iteration 268, loss = 0.19314965\n",
            "Iteration 269, loss = 0.19270285\n",
            "Iteration 270, loss = 0.19217476\n",
            "Iteration 271, loss = 0.19148533\n",
            "Iteration 272, loss = 0.19052414\n",
            "Iteration 273, loss = 0.18988658\n",
            "Iteration 274, loss = 0.18877595\n",
            "Iteration 275, loss = 0.18815791\n",
            "Iteration 276, loss = 0.18737493\n",
            "Iteration 277, loss = 0.18662854\n",
            "Iteration 278, loss = 0.18594219\n",
            "Iteration 279, loss = 0.18508061\n",
            "Iteration 280, loss = 0.18437022\n",
            "Iteration 281, loss = 0.18367582\n",
            "Iteration 282, loss = 0.18333830\n",
            "Iteration 283, loss = 0.18331068\n",
            "Iteration 284, loss = 0.18289216\n",
            "Iteration 285, loss = 0.18203084\n",
            "Iteration 286, loss = 0.18106919\n",
            "Iteration 287, loss = 0.18012626\n",
            "Iteration 288, loss = 0.17931195\n",
            "Iteration 289, loss = 0.17864410\n",
            "Iteration 290, loss = 0.17784003\n",
            "Iteration 291, loss = 0.17703905\n",
            "Iteration 292, loss = 0.17628491\n",
            "Iteration 293, loss = 0.17561733\n",
            "Iteration 294, loss = 0.17481263\n",
            "Iteration 295, loss = 0.17458939\n",
            "Iteration 296, loss = 0.17415103\n",
            "Iteration 297, loss = 0.17384954\n",
            "Iteration 298, loss = 0.17358846\n",
            "Iteration 299, loss = 0.17319056\n",
            "Iteration 300, loss = 0.17262030\n",
            "Iteration 301, loss = 0.17160828\n",
            "Iteration 302, loss = 0.17087199\n",
            "Iteration 303, loss = 0.17010341\n",
            "Iteration 304, loss = 0.16932544\n",
            "Iteration 305, loss = 0.16860318\n",
            "Iteration 306, loss = 0.16796145\n",
            "Iteration 307, loss = 0.16739562\n",
            "Iteration 308, loss = 0.16683922\n",
            "Iteration 309, loss = 0.16629758\n",
            "Iteration 310, loss = 0.16584007\n",
            "Iteration 311, loss = 0.16536299\n",
            "Iteration 312, loss = 0.16491374\n",
            "Iteration 313, loss = 0.16435527\n",
            "Iteration 314, loss = 0.16385836\n",
            "Iteration 315, loss = 0.16320704\n",
            "Iteration 316, loss = 0.16242045\n",
            "Iteration 317, loss = 0.16175651\n",
            "Iteration 318, loss = 0.16122612\n",
            "Iteration 319, loss = 0.16065888\n",
            "Iteration 320, loss = 0.15996786\n",
            "Iteration 321, loss = 0.15935412\n",
            "Iteration 322, loss = 0.15875912\n",
            "Iteration 323, loss = 0.15827814\n",
            "Iteration 324, loss = 0.15779758\n",
            "Iteration 325, loss = 0.15725360\n",
            "Iteration 326, loss = 0.15652553\n",
            "Iteration 327, loss = 0.15619311\n",
            "Iteration 328, loss = 0.15555348\n",
            "Iteration 329, loss = 0.15499456\n",
            "Iteration 330, loss = 0.15425547\n",
            "Iteration 331, loss = 0.15399473\n",
            "Iteration 332, loss = 0.15348952\n",
            "Iteration 333, loss = 0.15317704\n",
            "Iteration 334, loss = 0.15262979\n",
            "Iteration 335, loss = 0.15183629\n",
            "Iteration 336, loss = 0.15122059\n",
            "Iteration 337, loss = 0.15072781\n",
            "Iteration 338, loss = 0.15010786\n",
            "Iteration 339, loss = 0.14950474\n",
            "Iteration 340, loss = 0.14889894\n",
            "Iteration 341, loss = 0.14847357\n",
            "Iteration 342, loss = 0.14818872\n",
            "Iteration 343, loss = 0.14848128\n",
            "Iteration 344, loss = 0.14804315\n",
            "Iteration 345, loss = 0.14728983\n",
            "Iteration 346, loss = 0.14641786\n",
            "Iteration 347, loss = 0.14576705\n",
            "Iteration 348, loss = 0.14516118\n",
            "Iteration 349, loss = 0.14456085\n",
            "Iteration 350, loss = 0.14396673\n",
            "Iteration 351, loss = 0.14342119\n",
            "Iteration 352, loss = 0.14301789\n",
            "Iteration 353, loss = 0.14268256\n",
            "Iteration 354, loss = 0.14232550\n",
            "Iteration 355, loss = 0.14179035\n",
            "Iteration 356, loss = 0.14137558\n",
            "Iteration 357, loss = 0.14069264\n",
            "Iteration 358, loss = 0.14030903\n",
            "Iteration 359, loss = 0.14008971\n",
            "Iteration 360, loss = 0.13962733\n",
            "Iteration 361, loss = 0.13906273\n",
            "Iteration 362, loss = 0.13879325\n",
            "Iteration 363, loss = 0.13838187\n",
            "Iteration 364, loss = 0.13819419\n",
            "Iteration 365, loss = 0.13744834\n",
            "Iteration 366, loss = 0.13672849\n",
            "Iteration 367, loss = 0.13599472\n",
            "Iteration 368, loss = 0.13601610\n",
            "Iteration 369, loss = 0.13663500\n",
            "Iteration 370, loss = 0.13641280\n",
            "Iteration 371, loss = 0.13572538\n",
            "Iteration 372, loss = 0.13484876\n",
            "Iteration 373, loss = 0.13374138\n",
            "Iteration 374, loss = 0.13317069\n",
            "Iteration 375, loss = 0.13275411\n",
            "Iteration 376, loss = 0.13227577\n",
            "Iteration 377, loss = 0.13216939\n",
            "Iteration 378, loss = 0.13189064\n",
            "Iteration 379, loss = 0.13139431\n",
            "Iteration 380, loss = 0.13097066\n",
            "Iteration 381, loss = 0.13068092\n",
            "Iteration 382, loss = 0.13034095\n",
            "Iteration 383, loss = 0.13000227\n",
            "Iteration 384, loss = 0.12956733\n",
            "Iteration 385, loss = 0.12907688\n",
            "Iteration 386, loss = 0.12874554\n",
            "Iteration 387, loss = 0.12824784\n",
            "Iteration 388, loss = 0.12764701\n",
            "Iteration 389, loss = 0.12695833\n",
            "Iteration 390, loss = 0.12632609\n",
            "Iteration 391, loss = 0.12573598\n",
            "Iteration 392, loss = 0.12543302\n",
            "Iteration 393, loss = 0.12516623\n",
            "Iteration 394, loss = 0.12476758\n",
            "Iteration 395, loss = 0.12496124\n",
            "Iteration 396, loss = 0.12491387\n",
            "Iteration 397, loss = 0.12424094\n",
            "Iteration 398, loss = 0.12328463\n",
            "Iteration 399, loss = 0.12287457\n",
            "Iteration 400, loss = 0.12228545\n",
            "Iteration 401, loss = 0.12187945\n",
            "Iteration 402, loss = 0.12150204\n",
            "Iteration 403, loss = 0.12112098\n",
            "Iteration 404, loss = 0.12074138\n",
            "Iteration 405, loss = 0.12008365\n",
            "Iteration 406, loss = 0.11959941\n",
            "Iteration 407, loss = 0.11919314\n",
            "Iteration 408, loss = 0.11877562\n",
            "Iteration 409, loss = 0.11853486\n",
            "Iteration 410, loss = 0.11852158\n",
            "Iteration 411, loss = 0.11856100\n",
            "Iteration 412, loss = 0.11845389\n",
            "Iteration 413, loss = 0.11808139\n",
            "Iteration 414, loss = 0.11760917\n",
            "Iteration 415, loss = 0.11677723\n",
            "Iteration 416, loss = 0.11626412\n",
            "Iteration 417, loss = 0.11612178\n",
            "Iteration 418, loss = 0.11635087\n",
            "Iteration 419, loss = 0.11761419\n",
            "Iteration 420, loss = 0.11881817\n",
            "Iteration 421, loss = 0.11832412\n",
            "Iteration 422, loss = 0.11660733\n",
            "Iteration 423, loss = 0.11511721\n",
            "Iteration 424, loss = 0.11404431\n",
            "Iteration 425, loss = 0.11352866\n",
            "Iteration 426, loss = 0.11351614\n",
            "Iteration 427, loss = 0.11309411\n",
            "Iteration 428, loss = 0.11252450\n",
            "Iteration 429, loss = 0.11179941\n",
            "Iteration 430, loss = 0.11137896\n",
            "Iteration 431, loss = 0.11111759\n",
            "Iteration 432, loss = 0.11091421\n",
            "Iteration 433, loss = 0.11066908\n",
            "Iteration 434, loss = 0.11036834\n",
            "Iteration 435, loss = 0.10972754\n",
            "Iteration 436, loss = 0.10912526\n",
            "Iteration 437, loss = 0.10871090\n",
            "Iteration 438, loss = 0.10842988\n",
            "Iteration 439, loss = 0.10832456\n",
            "Iteration 440, loss = 0.10790203\n",
            "Iteration 441, loss = 0.10757285\n",
            "Iteration 442, loss = 0.10697197\n",
            "Iteration 443, loss = 0.10655309\n",
            "Iteration 444, loss = 0.10625343\n",
            "Iteration 445, loss = 0.10587794\n",
            "Iteration 446, loss = 0.10559125\n",
            "Iteration 447, loss = 0.10532635\n",
            "Iteration 448, loss = 0.10494844\n",
            "Iteration 449, loss = 0.10454836\n",
            "Iteration 450, loss = 0.10433784\n",
            "Iteration 451, loss = 0.10431124\n",
            "Iteration 452, loss = 0.10455585\n",
            "Iteration 453, loss = 0.10484882\n",
            "Iteration 454, loss = 0.10464329\n",
            "Iteration 455, loss = 0.10442278\n",
            "Iteration 456, loss = 0.10368911\n",
            "Iteration 457, loss = 0.10291478\n",
            "Iteration 458, loss = 0.10231970\n",
            "Iteration 459, loss = 0.10228201\n",
            "Iteration 460, loss = 0.10205300\n",
            "Iteration 461, loss = 0.10211955\n",
            "Iteration 462, loss = 0.10224258\n",
            "Iteration 463, loss = 0.10214557\n",
            "Iteration 464, loss = 0.10184883\n",
            "Iteration 465, loss = 0.10140384\n",
            "Iteration 466, loss = 0.10102830\n",
            "Iteration 467, loss = 0.10075679\n",
            "Iteration 468, loss = 0.10024316\n",
            "Iteration 469, loss = 0.09983642\n",
            "Iteration 470, loss = 0.09945861\n",
            "Iteration 471, loss = 0.09905811\n",
            "Iteration 472, loss = 0.09889967\n",
            "Iteration 473, loss = 0.09860489\n",
            "Iteration 474, loss = 0.09836654\n",
            "Iteration 475, loss = 0.09796639\n",
            "Iteration 476, loss = 0.09763706\n",
            "Iteration 477, loss = 0.09729682\n",
            "Iteration 478, loss = 0.09697771\n",
            "Iteration 479, loss = 0.09666154\n",
            "Iteration 480, loss = 0.09629444\n",
            "Iteration 481, loss = 0.09597406\n",
            "Iteration 482, loss = 0.09565907\n",
            "Iteration 483, loss = 0.09538868\n",
            "Iteration 484, loss = 0.09511059\n",
            "Iteration 485, loss = 0.09481059\n",
            "Iteration 486, loss = 0.09444170\n",
            "Iteration 487, loss = 0.09403969\n",
            "Iteration 488, loss = 0.09400104\n",
            "Iteration 489, loss = 0.09407855\n",
            "Iteration 490, loss = 0.09380617\n",
            "Iteration 491, loss = 0.09341578\n",
            "Iteration 492, loss = 0.09310305\n",
            "Iteration 493, loss = 0.09278799\n",
            "Iteration 494, loss = 0.09251050\n",
            "Iteration 495, loss = 0.09205850\n",
            "Iteration 496, loss = 0.09191312\n",
            "Iteration 497, loss = 0.09151426\n",
            "Iteration 498, loss = 0.09140427\n",
            "Iteration 499, loss = 0.09115299\n",
            "Iteration 500, loss = 0.09087104\n",
            "Iteration 501, loss = 0.09053846\n",
            "Iteration 502, loss = 0.09039619\n",
            "Iteration 503, loss = 0.09011592\n",
            "Iteration 504, loss = 0.08988037\n",
            "Iteration 505, loss = 0.08967902\n",
            "Iteration 506, loss = 0.08962599\n",
            "Iteration 507, loss = 0.08924577\n",
            "Iteration 508, loss = 0.08904015\n",
            "Iteration 509, loss = 0.08912468\n",
            "Iteration 510, loss = 0.08891704\n",
            "Iteration 511, loss = 0.08853385\n",
            "Iteration 512, loss = 0.08824959\n",
            "Iteration 513, loss = 0.08797114\n",
            "Iteration 514, loss = 0.08791976\n",
            "Iteration 515, loss = 0.08801404\n",
            "Iteration 516, loss = 0.08763135\n",
            "Iteration 517, loss = 0.08711063\n",
            "Iteration 518, loss = 0.08665666\n",
            "Iteration 519, loss = 0.08661632\n",
            "Iteration 520, loss = 0.08632719\n",
            "Iteration 521, loss = 0.08615394\n",
            "Iteration 522, loss = 0.08591089\n",
            "Iteration 523, loss = 0.08571254\n",
            "Iteration 524, loss = 0.08536997\n",
            "Iteration 525, loss = 0.08533860\n",
            "Iteration 526, loss = 0.08517389\n",
            "Iteration 527, loss = 0.08497893\n",
            "Iteration 528, loss = 0.08476509\n",
            "Iteration 529, loss = 0.08442403\n",
            "Iteration 530, loss = 0.08419365\n",
            "Iteration 531, loss = 0.08378491\n",
            "Iteration 532, loss = 0.08384225\n",
            "Iteration 533, loss = 0.08341583\n",
            "Iteration 534, loss = 0.08307679\n",
            "Iteration 535, loss = 0.08281059\n",
            "Iteration 536, loss = 0.08270517\n",
            "Iteration 537, loss = 0.08237000\n",
            "Iteration 538, loss = 0.08209550\n",
            "Iteration 539, loss = 0.08189858\n",
            "Iteration 540, loss = 0.08206219\n",
            "Iteration 541, loss = 0.08176664\n",
            "Iteration 542, loss = 0.08150769\n",
            "Iteration 543, loss = 0.08137811\n",
            "Iteration 544, loss = 0.08117019\n",
            "Iteration 545, loss = 0.08110420\n",
            "Iteration 546, loss = 0.08100577\n",
            "Iteration 547, loss = 0.08090577\n",
            "Iteration 548, loss = 0.08059457\n",
            "Iteration 549, loss = 0.08024866\n",
            "Iteration 550, loss = 0.07998974\n",
            "Iteration 551, loss = 0.07966995\n",
            "Iteration 552, loss = 0.07955945\n",
            "Iteration 553, loss = 0.07930095\n",
            "Iteration 554, loss = 0.07914685\n",
            "Iteration 555, loss = 0.07904441\n",
            "Iteration 556, loss = 0.07888572\n",
            "Iteration 557, loss = 0.07873870\n",
            "Iteration 558, loss = 0.07855063\n",
            "Iteration 559, loss = 0.07853706\n",
            "Iteration 560, loss = 0.07833102\n",
            "Iteration 561, loss = 0.07810071\n",
            "Iteration 562, loss = 0.07801685\n",
            "Iteration 563, loss = 0.07780290\n",
            "Iteration 564, loss = 0.07752509\n",
            "Iteration 565, loss = 0.07721854\n",
            "Iteration 566, loss = 0.07702249\n",
            "Iteration 567, loss = 0.07679611\n",
            "Iteration 568, loss = 0.07656022\n",
            "Iteration 569, loss = 0.07647500\n",
            "Iteration 570, loss = 0.07629110\n",
            "Iteration 571, loss = 0.07627618\n",
            "Iteration 572, loss = 0.07599237\n",
            "Iteration 573, loss = 0.07573661\n",
            "Iteration 574, loss = 0.07544201\n",
            "Iteration 575, loss = 0.07528036\n",
            "Iteration 576, loss = 0.07503840\n",
            "Iteration 577, loss = 0.07471061\n",
            "Iteration 578, loss = 0.07455351\n",
            "Iteration 579, loss = 0.07453341\n",
            "Iteration 580, loss = 0.07436091\n",
            "Iteration 581, loss = 0.07440611\n",
            "Iteration 582, loss = 0.07435270\n",
            "Iteration 583, loss = 0.07428577\n",
            "Iteration 584, loss = 0.07421716\n",
            "Iteration 585, loss = 0.07432396\n",
            "Iteration 586, loss = 0.07435896\n",
            "Iteration 587, loss = 0.07420915\n",
            "Iteration 588, loss = 0.07404886\n",
            "Iteration 589, loss = 0.07397655\n",
            "Iteration 590, loss = 0.07381296\n",
            "Iteration 591, loss = 0.07364306\n",
            "Iteration 592, loss = 0.07330288\n",
            "Iteration 593, loss = 0.07295123\n",
            "Iteration 594, loss = 0.07275579\n",
            "Iteration 595, loss = 0.07265252\n",
            "Iteration 596, loss = 0.07280645\n",
            "Iteration 597, loss = 0.07247989\n",
            "Iteration 598, loss = 0.07206561\n",
            "Iteration 599, loss = 0.07154024\n",
            "Iteration 600, loss = 0.07131509\n",
            "Iteration 601, loss = 0.07116770\n",
            "Iteration 602, loss = 0.07121026\n",
            "Iteration 603, loss = 0.07098100\n",
            "Iteration 604, loss = 0.07071343\n",
            "Iteration 605, loss = 0.07042278\n",
            "Iteration 606, loss = 0.07045006\n",
            "Iteration 607, loss = 0.07042967\n",
            "Iteration 608, loss = 0.07021364\n",
            "Iteration 609, loss = 0.06995985\n",
            "Iteration 610, loss = 0.06970549\n",
            "Iteration 611, loss = 0.06950991\n",
            "Iteration 612, loss = 0.06931244\n",
            "Iteration 613, loss = 0.06918467\n",
            "Iteration 614, loss = 0.06898058\n",
            "Iteration 615, loss = 0.06881126\n",
            "Iteration 616, loss = 0.06857822\n",
            "Iteration 617, loss = 0.06872726\n",
            "Iteration 618, loss = 0.06840986\n",
            "Iteration 619, loss = 0.06821736\n",
            "Iteration 620, loss = 0.06806769\n",
            "Iteration 621, loss = 0.06799023\n",
            "Iteration 622, loss = 0.06802018\n",
            "Iteration 623, loss = 0.06806124\n",
            "Iteration 624, loss = 0.06794472\n",
            "Iteration 625, loss = 0.06778071\n",
            "Iteration 626, loss = 0.06785425\n",
            "Iteration 627, loss = 0.06794979\n",
            "Iteration 628, loss = 0.06776049\n",
            "Iteration 629, loss = 0.06737333\n",
            "Iteration 630, loss = 0.06692546\n",
            "Iteration 631, loss = 0.06684537\n",
            "Iteration 632, loss = 0.06706200\n",
            "Iteration 633, loss = 0.06713765\n",
            "Iteration 634, loss = 0.06696477\n",
            "Iteration 635, loss = 0.06658867\n",
            "Iteration 636, loss = 0.06642088\n",
            "Iteration 637, loss = 0.06601358\n",
            "Iteration 638, loss = 0.06591898\n",
            "Iteration 639, loss = 0.06580551\n",
            "Iteration 640, loss = 0.06581902\n",
            "Iteration 641, loss = 0.06582050\n",
            "Iteration 642, loss = 0.06576730\n",
            "Iteration 643, loss = 0.06562235\n",
            "Iteration 644, loss = 0.06534133\n",
            "Iteration 645, loss = 0.06521758\n",
            "Iteration 646, loss = 0.06521601\n",
            "Iteration 647, loss = 0.06564598\n",
            "Iteration 648, loss = 0.06530833\n",
            "Iteration 649, loss = 0.06481911\n",
            "Iteration 650, loss = 0.06463238\n",
            "Iteration 651, loss = 0.06453546\n",
            "Iteration 652, loss = 0.06438134\n",
            "Iteration 653, loss = 0.06439973\n",
            "Iteration 654, loss = 0.06427773\n",
            "Iteration 655, loss = 0.06407467\n",
            "Iteration 656, loss = 0.06392891\n",
            "Iteration 657, loss = 0.06374104\n",
            "Iteration 658, loss = 0.06370138\n",
            "Iteration 659, loss = 0.06348328\n",
            "Iteration 660, loss = 0.06356460\n",
            "Iteration 661, loss = 0.06369383\n",
            "Iteration 662, loss = 0.06409021\n",
            "Iteration 663, loss = 0.06421246\n",
            "Iteration 664, loss = 0.06395914\n",
            "Iteration 665, loss = 0.06340288\n",
            "Iteration 666, loss = 0.06272084\n",
            "Iteration 667, loss = 0.06242057\n",
            "Iteration 668, loss = 0.06206507\n",
            "Iteration 669, loss = 0.06186233\n",
            "Iteration 670, loss = 0.06175101\n",
            "Iteration 671, loss = 0.06172366\n",
            "Iteration 672, loss = 0.06149730\n",
            "Iteration 673, loss = 0.06145016\n",
            "Iteration 674, loss = 0.06122838\n",
            "Iteration 675, loss = 0.06118265\n",
            "Iteration 676, loss = 0.06117995\n",
            "Iteration 677, loss = 0.06113368\n",
            "Iteration 678, loss = 0.06107867\n",
            "Iteration 679, loss = 0.06102724\n",
            "Iteration 680, loss = 0.06111340\n",
            "Iteration 681, loss = 0.06120455\n",
            "Iteration 682, loss = 0.06129542\n",
            "Iteration 683, loss = 0.06123423\n",
            "Iteration 684, loss = 0.06132538\n",
            "Iteration 685, loss = 0.06103366\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rede_neural = MLPClassifier(max_iter=1000, verbose=True, tol=0.00000000000001, solver = 'adam', activation = 'relu', hidden_layer_sizes = 9)\n",
        "modelo.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-S4o3IczVP"
      },
      "source": [
        "\n",
        "\n",
        "> **Vamos testar o modelo?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "1q9nsbSjdu23"
      },
      "outputs": [],
      "source": [
        "previsoes = modelo.predict(X_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0PlSJE8fAUL",
        "outputId": "501c2371-84dd-4c64-f216-be2b03707888"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events',\n",
              "       'no-recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events', 'recurrence-events', 'recurrence-events',\n",
              "       'no-recurrence-events', 'no-recurrence-events',\n",
              "       'recurrence-events', 'no-recurrence-events', 'recurrence-events',\n",
              "       'recurrence-events'], dtype='<U20')"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previsoes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjWziqc5fV8m"
      },
      "source": [
        "\n",
        "\n",
        "> **Será se o modelo acertou?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q92H3KOtfN5E",
        "outputId": "864bf803-aaec-436b-ee87-2bd4956fc72e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "285    no-recurrence-events\n",
              "281    no-recurrence-events\n",
              "33     no-recurrence-events\n",
              "211       recurrence-events\n",
              "93     no-recurrence-events\n",
              "               ...         \n",
              "228    no-recurrence-events\n",
              "371       recurrence-events\n",
              "176    no-recurrence-events\n",
              "272    no-recurrence-events\n",
              "3      no-recurrence-events\n",
              "Name: Class, Length: 81, dtype: object"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ9MxYOIfmwv",
        "outputId": "d81573e1-45aa-4ea6-d922-c25e1a623343"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6790123456790124"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "accuracy_score(y_teste,previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3D5bvushr9W",
        "outputId": "45328eb3-d2be-4d4c-c5e1-248e262d7d9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[31, 16],\n",
              "       [10, 24]])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion_matrix(y_teste, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "wX15YT-7j-c9",
        "outputId": "db8b53f9-e72b-4257-c955-a229ab056d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6790123456790124"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(modelo)\n",
        "cm.fit(X_treino, y_treino)\n",
        "cm.score(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIixOPw1kw-z",
        "outputId": "5b4735c0-0e3f-4981-b422-f4a5d25f5c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                      precision    recall  f1-score   support\n",
            "\n",
            "no-recurrence-events       0.76      0.66      0.70        47\n",
            "   recurrence-events       0.60      0.71      0.65        34\n",
            "\n",
            "            accuracy                           0.68        81\n",
            "           macro avg       0.68      0.68      0.68        81\n",
            "        weighted avg       0.69      0.68      0.68        81\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_teste, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### hyperparameter tuning using GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=1000, random_state=42),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;, &#x27;logistic&#x27;],\n",
              "                         &#x27;hidden_layer_sizes&#x27;: [(10,), (20,), (30,), (10, 5),\n",
              "                                                (20, 10), (30, 15)]},\n",
              "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=1000, random_state=42),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;, &#x27;logistic&#x27;],\n",
              "                         &#x27;hidden_layer_sizes&#x27;: [(10,), (20,), (30,), (10, 5),\n",
              "                                                (20, 10), (30, 15)]},\n",
              "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=1000, random_state=42),\n",
              "             n_jobs=-1,\n",
              "             param_grid={'activation': ['relu', 'tanh', 'logistic'],\n",
              "                         'hidden_layer_sizes': [(10,), (20,), (30,), (10, 5),\n",
              "                                                (20, 10), (30, 15)]},\n",
              "             scoring='accuracy')"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,), (20,), (30,), (10, 5), (20, 10), (30, 15)],\n",
        "    'activation': ['relu', 'tanh', 'logistic'],\n",
        "}\n",
        "\n",
        "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'activation': 'relu', 'hidden_layer_sizes': (30,)}\n"
          ]
        }
      ],
      "source": [
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_treino, y_treino)\n",
        "predictions = best_model.predict(X_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the best model: 0.7654320987654321\n"
          ]
        }
      ],
      "source": [
        "\n",
        "accuracy = accuracy_score(y_teste, predictions)\n",
        "print(\"Accuracy of the best model:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
